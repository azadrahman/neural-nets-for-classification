\begin{thebibliography}{9}


\bibitem{higham}
C.~Higham and D.~Higham.
\newblock Deep Learning: An Introduction for Applied Mathematicians.
\newblock {\em arXiv:1801.05894 [math.HO]}, 2018.


\bibitem{lecun_backprop}
Y.~LeCun, L.~Bottou, G.~Orr and K.~Muller.
\newblock Efficient BackProp.
\newblock In {\em Neural Networks: Tricks of the trade}, Springer, 1998.


\bibitem{line_search}
P.~Stanimirovic and M.~Miladinovic.
\newblock Accelerated gradient descent methods with line search.
\newblock In {\em Numerical Algorithms} 54:503-520, Springer, 2010.


\bibitem{adam}
D.~Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv:1412.6980 [cs.LG]}, 2014.


\bibitem{bionets}
A.~Thomas and C. Kaltschmidt.
\newblock Bio-inspired Neural Networks.
\newblock In {\em Memristor Networks}, Springer, 2014.


\bibitem{hinton_backprop}
D.~Rumelhart, G.~Hinton and R. Williams.
\newblock Learning representations by back-propagating errors.
\newblock In {\em Nature} 323:533-536, 1986.


\bibitem{early_stopping}
L.~Prechelt.
\newblock Early Stopping â€” But When?.
\newblock In {\em Neural Networks: Tricks of the Trade}. Lecture Notes in Computer Science, vol 7700. Springer, 2012.


\bibitem{kmnist}
T.~Clanuwat et~al.
\newblock Deep Learning for Classical Japanese Literature.
\newblock {\em arXiv:1812.01718 [cs.CV]}, 2018.


\end{thebibliography}
