\documentclass{article}
\usepackage{arxiv}

\usepackage{amssymb, amsmath}
\usepackage{amsthm}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\setlength{\jot}{15pt}

\usepackage{graphicx}
\graphicspath{graphics/}

\begin{document}
	
	
	\title{Neural Networks for Classification}
	\author{Owen Jones (olj23\@bath.ac.uk)}
	\date{Spring 2019}
	\maketitle


\begin{abstract}
	Here is the contents of my abstract blah blah blah blah blah.
\end{abstract}



\section{Introduction to Classification Problems}

    Historically, machine learning tasks have fallen into one of several categories: supervised learning problems, unsupervised learning problems, and more recently reinforcement learning
    
    Classification tasks (in the sense which this paper considers) belong to the supervised learning family of tasks, where the training process uses
    
    qualitative outputs
    
    In Section \ref{sec:demos} of this paper, the 


\section{Logistic Regression}

    

    \subsection{Introducing Nonlinearity}

    \subsection{The Cost Function}

    \subsection{Regularisation}



\section{Neural Networks}

    \subsection{Multiple Outputs}
    
    \subsection{Forward Propagation}

    \subsection{Backpropagation}
        
        
        $$ \begin{aligned}
        J(\theta) &= - \sum_{k=0}^{n} \left( y_k \log(a^{(L)}_k) + (1 - y_k) \log(1 - a^{(L)}_k) \right) \\
            &= -\mathbf{y}^\top \log(\mathbf{a}^{(L)}) - (\mathbf{1}-\mathbf{y})^\top \log(\mathbf{1} - \mathbf{a}^{(L)})
        \end{aligned} $$
        
        
        
        $$
        \frac{\partial J}{\partial \theta^{(L-1)}} =
            \frac{\partial J}{\partial \mathbf{a}^{(L)}} \
            \frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{z}^{(L)}} \
            \frac{\partial \mathbf{z}^{(L)}}{\partial \theta^{(L-1)}}
        $$



        $$
        \frac{\partial J}{\partial \theta^{(L-2)}} =
            \frac{\partial J}{\partial \mathbf{a}^{(L)}} \
            \frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{z}^{(L)}} \
            \frac{\partial \mathbf{z}^{(L)}}{\partial \mathbf{a}^{(L-1)}} \
            \frac{\partial \mathbf{a}^{(L-1)}}{\partial \mathbf{z}^{(L-1)}} \
            \frac{\partial \mathbf{z}^{(L-1)}}{\partial \mathbf{a}^{(L-2)}}
        $$



        Where we define the "error" of layer $l$ as
    
        $$
        \mathbf{\delta}^{(L)} :=  \frac{\partial J}{\partial \mathbf{z}^{(l)}} \in \mathbb{R}^{n_l}
        $$
        
        The $j$th component of this error vector is a measure of the sensitivity of the cost function $J(\theta)$ to the (weighted) input from neuron $j$ in layer $l$. Although it is \textit{not} a direct measure of how much a particular neuron is at fault for mistakes in the network's eventual output, such an explanation is at least valid intuitively, since all partial derivatives must be zero if the cost function is at a minimum.
        
        Note that when $J(\theta)$ is the cross entropy cost function, the error in the final layer $L$ can be calculated as follows:
        
        \begin{lemma}
            If $\mathbf{a}^{(L)} = \sigma(\mathbf{z}^{(L)})$ is the activated output of the final layer $L$ of the network, where $\sigma$ is either the sigmoid or softmax activation function, then
            
            $$
            \mathbf{\delta}^{(L)} = \mathbf{a}^{(L)} - \mathbf{y}
            $$
            
        \end{lemma}
        
        
        \begin{proof}
            
            Firstly, where $\mathbf{y}$ is the expected output of the network, observe that the partial derivative of $J$ with respect to the $j$th component of the activated output $\mathbf{a}^{(L)}$ can be represented as
                    
            $$ \begin{aligned}
            \frac{\partial J(\theta)}{\partial a^{(L)}_j}
            &= \frac{\partial}{\partial a^{(L)}_i} \left( - \sum_{k=0}^{n_L} \left( y_k \log(a^{(L)}_k) + (1 - y_k) \log(1 - a^{(L)}_k) \right) \right) \\
            &= - \frac{y_j}{a^{(L)}_j} + \frac{1 - y_j}{1 - a^{(L)}_j}
            \end{aligned} $$
            
            Consider now the partial derivative of the activated output of layer $L$, with respect to the non-activated output; that is,
            
            $$
            \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j} = \frac{\partial}{\partial z^{(L)}_j} \left( \sigma(z^{(L)}_j) \right)
            $$
            
             
            \begin{itemize}
                
                \item When $\sigma$ is the sigmoid activation function,
            
                $$
                \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j} = \frac{\partial}{\partial z^{(L)}_j} \left( \frac{1}{1 + e^{-z^{(L)}_i}} \right)
                $$
                
                Note that if $i \neq j$ then this is clearly zero. If $i = j$,
                
                $$ \begin{aligned}
                \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j}
                &= \left( - e^{-z^{(L)}_j} \right) \left( - \left( 1 + e^{-z^{(L)}_j} \right)^{-2} \right) \\
                &= \frac{e^{-z^{(L)}_j}}{\left( 1 + e^{-z^{(L)}_j} \right) \left( 1 + e^{-z^{(L)}_j} \right)} \\
                &= \left( \frac{1}{1 + e^{-z^{(L)}_j}} \right) \left( \frac{\left( 1 + e^{-z^{(L)}_j} \right) - 1}{1 + e^{-z^{(L)}_j}} \right) \\
                &= \sigma(z^{(L)}_j) \left( 1 - \sigma(z^{(L)}_j) \right)
                \end{aligned} $$
                
            
            
                \item When $\sigma$ is the softmax activation function, then using the quotient rule,
                
                $$ \begin{aligned}
                \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j}
                &= \frac{\partial}{\partial z^{(L)}_j} \left( \frac{e^{z^{(L)}_i}}{\sum_{k=0}^{n_L} e^{z^{(L)}_k}} \right) \\
                &= \frac{\left( \frac{\partial \left( e^{z^{(L)}_i}\right)}{\partial z^{(L)}_j} \cdot \sum_{k=0}^{n_L} e^{z^{(L)}_k} \right) - e^{z^{(L)}_i} e^{z^{(L)}_j}}{\left( \sum_{k=0}^{n_L} e^{z^{(L)}_k} \right)^2}
                \end{aligned} $$
                
                
                If $i \neq j$,
               
                $$ \begin{aligned}
                \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j}
                &= \frac{0 - e^{z_i} e^{z_j}}{\left( \sum_{k=0}^{n_L} e^{z^{(L)}_k} \right)^2} \\
                &= - \sigma(z_i) \sigma(z_j)
                \end{aligned} $$
                
                On the other hand, if $i = j$,
                
                $$ \begin{aligned}
                \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j}
                &= \frac{\left( e^{z^{(L)}_j} \cdot \sum_{k=0}^{n_L} e^{z^{(L)}_k} \right) - \left( e^{z^{(L)}_j} \right)^2}{\left( \sum_{k=0}^{n_L} e^{z^{(L)}_k} \right)^2} \\
                &= \frac{e^{z^{(L)}_j}}{\sum_{k=0}^{n_L} e^{z^{(L)}_k}} \left( 1 - \frac{e^{z^{(L)}_j}}{\sum_{k=0}^{n_L} e^{z^{(L)}_k}} \right) \\
                &= \sigma(z^{(L)}_j) \left( 1 - \sigma(z^{(L)}_j) \right)
                \end{aligned} $$
                
            \end{itemize}
            
            Therefore in both cases, we have that $\frac{\partial a^{(L)}_j}{\partial z^{(L)}_j} = \sigma(z^{(L)}_j) \left( 1 - \sigma(z^{(L)}_j) \right)$.
            
            Now using the definition of $\mathbf{\delta}^{(L)}$ and the chain rule, and combining the two previous results,
            
            $$ \begin{aligned}
            \delta^{(L)}_j &= \frac{\partial J(\theta)}{\partial z^{(L)}_j} \\
            &= \sum_{i = 0}^{n_L} \left( \frac{\partial J(\theta)}{\partial a^{(L)}_i} \ \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j} \right) \\
            &= \left( - \frac{y_j}{a^{(L)}_j} + \frac{1 - y_j}{1 - a^{(L)}_j} \right) \sigma(z^{(L)}_j) \left( 1 - \sigma(z^{(L)}_j) \right)
            \end{aligned} $$
            
            Then recalling that $\sigma(z^{(L)}_j) = a^{(L)}_j$ by definition,
            
            $$ \begin{aligned}
            \delta^{(L)}_j &= \left( - \frac{y_j}{a^{(L)}_j} + \frac{1 - y_j}{1 - a^{(L)}_j} \right) a^{(L)}_j \left( 1 - a^{(L)}_j \right) \\
            &= a^{(L)}_j (1 - y_j) - y_j (1 - a^{(L)}_j) \\
            &= a^{(L)}_j - y_j
            \end{aligned} $$
            
            % TODO this is fine for sigmoid, but not softmax
    
    
    
        \end{proof}



% TODO incorporate this for softmax?

$$ \begin{cases}
\left( - \frac{y_j}{a^{(L)}_j} + \frac{1 - y_j}{1 - a^{(L)}_j} \right) \sigma(z^{(L)}_j) \left( 1 - \sigma(z^{(L)}_j) \right) + \sum_{i=0}^{n_L} \left( \left( - \frac{y_i}{a^{(L)}_i} + \frac{1 - y_i}{1 - a^{(L)}_i} \right) \left( - \sigma(z_i) \sigma(z_j) \right) \right) & \text{for softmax activation}
\end{cases}$$
        
        
        Now
        
        
        
        $$
        \delta^{(l)} :=
        \delta^{(l+1)} \
        \frac{\partial \mathbf{z}^{(l+1)}}{\partial \mathbf{a}^{(l)}} \
        \frac{\partial \mathbf{a}^{(l)}}{\partial \mathbf{z}^{(l)}} \
        $$
        
        
        
        $$
        \delta^{(l)} :=
            \delta^{(l+1)} \
            \frac{\partial \mathbf{z}^{(l+1)}}{\partial \mathbf{a}^{(l)}} \
            \frac{\partial \mathbf{a}^{(l)}}{\partial \mathbf{z}^{(l)}} \
        $$
        
        
        
        
        
        






\section{Demonstration of Methods}
\label{sec:demos}

    \subsection{Binary Classification}
    
    \subsection{Multiclass Classification}
    



\section{Areas for Further Work}    



\bibliography{report}
\bibliographystyle{abbrv}

\end{document}
	
	