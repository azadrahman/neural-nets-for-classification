\documentclass{article}[11pt]
\usepackage{arxiv}
\usepackage{url}

\usepackage{amssymb, amsmath}
\usepackage{amsthm}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\setlength{\jot}{12pt}

\usepackage{graphicx}
\graphicspath{graphics/}

\begin{document}
	
	
	\title{Neural Networks for Classification}
	\author{Owen Jones (olj23\@bath.ac.uk)}
	\date{Spring 2019}
	\maketitle


\begin{abstract}
	Here is the contents of my abstract blah blah blah blah blah.
\end{abstract}



\section{Introduction to Classification Problems}

    Historically, machine learning tasks have fallen into one of several categories: supervised learning problems, unsupervised learning problems, and more recently reinforcement learning
    
    Classification tasks (in the sense which this paper considers) belong to the supervised learning family of tasks, where the data used in the training process is \textit{labelled}
    
    qualitative outputs
    
    In Section \ref{sec:demos} of this paper, the 


\section{Logistic Regression}

    binary output (0 or 1)


    \subsection{Linear Regression???}
        
        The underlying assumption is always that the output occurs as a result of some combination of inputs.
        
        Mathematically speaking, suppose we have an observation $\mathbf{x}$ consisting of $n \geq 1$ inputs:
        
        $$
        \mathbf{x} = (x_1, \ x_2, \ \ldots, \ x_{n-1}, \ x_n) \in \mathbb{R}^n
        $$
        
        Then we assume that there exists a corresponding set of weights $\theta_0, \ldots, \theta_n \in \mathbb{R}$ such that
        
        $$
        z = \theta_0 + \sum_{k=1}^n \theta_k x_k
        $$
        
        %TODO

        
        Therefore the output of linear regression is some number $z \in (-\infty, \infty)$
        
        
    \subsection{Introducing Nonlinearity}
        
        Recall that the goal of a binary classification task is to assign a categorical label to each observation; so if we wish to use linear regression as the basis for our algorithm, ultimately we will need to somehow interpret the continuous output $z \in \mathbb{R}$ as corresponding to one of those labels.
        
        This is achieved by means of an \textit{activation function}:
        
        $$
        \sigma \colon (-\infty, \infty) \to (0, 1)
        $$
        
        In later sections we will consider other activation functions, but for logistic regression we typically use the \textit{sigmoid} function,
            
        $$
        \sigma(z) = \frac{1}{1 + e^{-z}}
        $$
        
        
        % TODO sigmoid plot
        
        
        
            
        
        
        
        
        


    \subsection{The Cost Function}
        
        
        
        Here we introduce the \textit{binomial cross-entropy} cost function:
        
        $$
        J(\theta) = - \frac{1}{m} \sum_{k=1}^{m} \left( y_k \log(a_k) + (1 - y_k) \log(1 - a_k) \right)
        $$

    
    \subsection{Regularisation}
    
    \subsection{Training}
    
    % TODO
    Hyperparams
    Variable learning rate
    Validation set
    Early stopping



\section{Neural Networks}

    \subsection{Multiple Outputs}
    
    
    
            
    Shifted softmax
    
    \subsection{Forward Propagation}

    \subsection{Backpropagation}
        
        
        $$ \begin{aligned}
        J(\theta)
        &= - \sum_{k=1}^{n} \left( y_k \log(a^{(L)}_k) + (1 - y_k) \log(1 - a^{(L)}_k) \right) \\
        &= -\mathbf{y}^\top \log(\mathbf{a}^{(L)}) - (\mathbf{1}-\mathbf{y})^\top \log(\mathbf{1} - \mathbf{a}^{(L)})
        \end{aligned} $$
        
        
        
        $$
        \frac{\partial J}{\partial \theta^{(L-1)}} =
            \frac{\partial J}{\partial \mathbf{a}^{(L)}} \
            \frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{z}^{(L)}} \
            \frac{\partial \mathbf{z}^{(L)}}{\partial \theta^{(L-1)}}
        $$



        $$
        \frac{\partial J}{\partial \theta^{(L-2)}} =
            \frac{\partial J}{\partial \mathbf{a}^{(L)}} \
            \frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{z}^{(L)}} \
            \frac{\partial \mathbf{z}^{(L)}}{\partial \mathbf{a}^{(L-1)}} \
            \frac{\partial \mathbf{a}^{(L-1)}}{\partial \mathbf{z}^{(L-1)}} \
            \frac{\partial \mathbf{z}^{(L-1)}}{\partial \mathbf{a}^{(L-2)}}
        $$



        Where we define the "error" of layer $l$ as
    
        $$
        \mathbf{\delta}^{(l)} :=  \frac{\partial J}{\partial \mathbf{z}^{(l)}} \in \mathbb{R}^{n_l}
        $$
        
        The $j$th component of this error vector is a measure of the sensitivity of the cost function $J(\theta)$ to the (weighted) input from neuron $j$ in layer $l$. Although it is \textit{not} a direct measure of how much a particular neuron is at fault for mistakes in the network's eventual output, such an explanation is at least valid intuitively, since all partial derivatives must be zero if the cost function is at a minimum.
        
        
        % TODO wording!!!
        Now define recursively... for $l = 1, \ldots, L-1$
        
        $$
        \delta^{(l)} =
        \delta^{(l+1)} \
        \frac{\partial \mathbf{z}^{(l+1)}}{\partial \mathbf{a}^{(l)}} \
        \frac{\partial \mathbf{a}^{(l)}}{\partial \mathbf{z}^{(l)}} \
        $$
        
        
        
        % TODO reword this - binary/multiclass?
        Note that when $J(\theta)$ is the cross-entropy cost function, the error in the final layer $L$ can be calculated as follows:
        
        \begin{lemma}
            
            If $J$ is the \textbf{binomial cross-entropy} cost function, and $\sigma$ is the \textbf{sigmoid} activation function, then for all $j = 1, \ \ldots, \ n_L$
            
            $$
            \delta^{(L)}_j = a^{(L)}_j - y_j
            $$
            
        \end{lemma}
        
        
        \begin{proof}
            
            Observe that the partial derivative of $J$ with respect to $a^{(L)}_i$ (the $i$th component of the activated output $\mathbf{a}^{(L)}$) can be represented as
                    
            $$ \begin{aligned}
            \frac{\partial J(\theta)}{\partial a^{(L)}_i}
            &= \frac{\partial}{\partial a^{(L)}_i} \left( - \sum_{k=1}^{n_L} \left( y_k \log(a^{(L)}_k) + (1 - y_k) \log(1 - a^{(L)}_k) \right) \right) \\
            &= - \frac{y_i}{a^{(L)}_i} + \frac{1 - y_i}{1 - a^{(L)}_i}
            \end{aligned} $$
            
            Consider now the partial derivative of the activated output of layer $L$, with respect to the non-activated output; that is,
            
            $$ \begin{aligned}
            \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j}
            &= \frac{\partial}{\partial z^{(L)}_j} \left( \sigma(z^{(L)}_j) \right)
            &= \frac{\partial}{\partial z^{(L)}_j} \left( \frac{1}{1 + e^{-z^{(L)}_i}} \right)
            \end{aligned} $$
                
            Note that if $i \neq j$ then this is clearly zero. If $i = j$,
            
            $$ \begin{aligned}
            \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j}
            &= \left( - e^{-z^{(L)}_j} \right) \left( - \left( 1 + e^{-z^{(L)}_j} \right)^{-2} \right) \\
            &= \frac{e^{-z^{(L)}_j}}{\left( 1 + e^{-z^{(L)}_j} \right) \left( 1 + e^{-z^{(L)}_j} \right)} \\
            &= \left( \frac{1}{1 + e^{-z^{(L)}_j}} \right) \left( \frac{\left( 1 + e^{-z^{(L)}_j} \right) - 1}{1 + e^{-z^{(L)}_j}} \right) \\
            &= \sigma(z^{(L)}_j) \left( 1 - \sigma(z^{(L)}_j) \right) \\
            &= a^{(L)}_j \left( 1 - a^{(L)}_j \right)
            \end{aligned} $$
            
            Now using the definition of $\boldsymbol{\delta}^{(L)}$ and the chain rule,
            
            $$ \begin{aligned}
            \delta^{(L)}_j &= \frac{\partial J(\theta)}{\partial z^{(L)}_j} \\
            &= \sum_{i=1}^{n_L} \left( \frac{\partial J(\theta)}{\partial a^{(L)}_i} \ \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j} \right) \\
            &= \frac{\partial J(\theta)}{\partial a^{(L)}_j} \ \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j} + \sum_{\substack{i=1 \\ i \neq j}}^{n_L} \left( \frac{\partial J(\theta)}{\partial a^{(L)}_i} \ \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j} \right) \\
            &= \frac{\partial J(\theta)}{\partial a^{(L)}_j} \ \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j} + 0 \\
            &= \left( - \frac{y_j}{a^{(L)}_j} + \frac{1 - y_j}{1 - a^{(L)}_j} \right) a^{(L)}_j \left( 1 - a^{(L)}_j \right) \\
            &= a^{(L)}_j (1 - y_j) - y_j (1 - a^{(L)}_j) \\
            &= a^{(L)}_j - y_j
            \end{aligned} $$
            
        \end{proof}
        
        
        % TODO word nicely
        We now complete a similar proof for the softmax case.
        
        
        \begin{lemma}
            
            If $J$ is the \textbf{multiclass cross-entropy} cost function, and $\sigma$ is the \textbf{softmax} activation function, then for all $j = 1, \ \ldots, \ n_L$
            
            $$
            \delta^{(L)}_j = a^{(L)}_j - y_j
            $$
            
        \end{lemma}
        
        
        \begin{proof}
            
            Firstly note that
            
            $$ \begin{aligned}
            \frac{\partial J(\theta)}{\partial a^{(L)}_i}
            &= \frac{\partial}{\partial a^{(L)}_i} \left( - \sum_{k=1}^{n_L} y_k \log(a^{(L)}_k) \right) \\
            &= - \frac{y_i}{a^{(L)}_i}
            \end{aligned} $$
            
            
            Then using the quotient rule,
            
            $$ \begin{aligned}
            \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j}
            &= \frac{\partial}{\partial z^{(L)}_j} \left( \frac{e^{z^{(L)}_i}}{\sum_{k=1}^{n_L} e^{z^{(L)}_k}} \right) \\
            &= \frac{\left( \frac{\partial \left( e^{z^{(L)}_i}\right)}{\partial z^{(L)}_j} \cdot \sum_{k=1}^{n_L} e^{z^{(L)}_k} \right) - e^{z^{(L)}_i} e^{z^{(L)}_j}}{\left( \sum_{k=1}^{n_L} e^{z^{(L)}_k} \right)^2}
            \end{aligned} $$
            
            \begin{itemize}
            
                \item If $i \neq j$,
            
                $$ \begin{aligned}
                \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j}
                &= \frac{0 - e^{z_i} e^{z_j}}{\left( \sum_{k=1}^{n_L} e^{z^{(L)}_k} \right)^2} \\
                &= - \sigma(z_i) \sigma(z_j) \\
                &= - a^{(L)}_i a^{(L)}_j
                \end{aligned} $$
            
                \item On the other hand, if $i = j$,
            
                $$ \begin{aligned}
                \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j}
                &= \frac{\left( e^{z^{(L)}_j} \cdot \sum_{k=1}^{n_L} e^{z^{(L)}_k} \right) - \left( e^{z^{(L)}_j} \right)^2}{\left( \sum_{k=1}^{n_L} e^{z^{(L)}_k} \right)^2} \\
                &= \frac{e^{z^{(L)}_j}}{\sum_{k=1}^{n_L} e^{z^{(L)}_k}} \left( 1 - \frac{e^{z^{(L)}_j}}{\sum_{k=1}^{n_L} e^{z^{(L)}_k}} \right) \\
                &= \sigma(z^{(L)}_j) \left( 1 - \sigma(z^{(L)}_j) \right) \\
                &= a^{(L)}_j \left( 1 - a^{(L)}_j \right)
                \end{aligned} $$
            
            \end{itemize}
            
            Now once again using the definition of $\boldsymbol{\delta}^{(L)}$ and the chain rule, observe that
            
            $$ \begin{aligned}
            \delta^{(L)}_j &= \frac{\partial J(\theta)}{\partial z^{(L)}_j} \\
            &= \sum_{i=1}^{n_L} \left( \frac{\partial J(\theta)}{\partial a^{(L)}_i} \ \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j} \right) \\
            &= \frac{\partial J(\theta)}{\partial a^{(L)}_j} \ \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j} + \sum_{\substack{i=1 \\ i \neq j}}^{n_L} \left( \frac{\partial J(\theta)}{\partial a^{(L)}_i} \ \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j} \right) \\
            &= - \frac{y_j}{a^{(L)}_j} \left( a^{(L)}_j (1 - a^{(L)}_j) \right) + \sum_{\substack{i=1 \\ i \neq j}}^{n_L} \left( - \frac{y_i}{a^{(L)}_i} \left(- a^{(L)}_i a^{(L)}_j \right) \right) \\
            &= - y_j (1 - a^{(L)}_j) + a^{(L)}_j \left( \sum_{i=1}^{n_L} \left( y_i \right) - y_j \right)
            \end{aligned} $$
            
            This looks slightly complicated, but recall that $\mathbf{y}$ is a one-hot encoded vector, and therefore $\sum_{=1}^{n_L} y_i = 1$. Hence we have
            
            $$ \begin{aligned}
            \delta^{(L)}_j &= - y_j (1 - a^{(L)}_j) + a^{(L)}_j (1 - y_j) \\
            &= a^{(L)}_j - y_j
            \end{aligned} $$
            
        \end{proof}
    
        
        
        \begin{lemma}
            
            For all $i = 1, \ \ldots, \ n_l$ and all $j = 1, \ \ldots, \ n_{l-1}$,
            
            $$
            \frac{\partial z^{(l)}_i}{\partial a^{(l-1)}_j} = \theta^{(l-1)}_{i,j}
            $$
            
        \end{lemma}
        
        \begin{proof}
            
            By the definition of $z^{(l)}_i$, we have
            
            $$ \begin{aligned}
            \frac{\partial z^{(l)}_i}{\partial a^{(l-1)}_j}
            &= \frac{\partial}{\partial a^{(l-1)}_j} \left( \sum_{k = 1}^{n_{l-1}} a^{(l-1)}_k \theta^{(l-1)}_{i,k} \right) \\
            &= \theta^{(l-1)}_{i,j}
            \end{aligned} $$
            
        \end{proof}
        
        
        
        
        \begin{lemma}
            
            For all $i = 1, \ \ldots, \ n_l$ and all $j = 1, \ \ldots, \ n_{l-1}$,
            
            $$
            \frac{\partial z^{(l)}_p}{\partial \theta^{(l-1)}_{i,j}} = \begin{cases}
                0 & p \neq i \\
                a^{(l-1)}_j & p = i
            \end{cases}
            $$
            
        \end{lemma}
        
        \begin{proof}
            
            Using the definition of $z^{(l)}_p$,
            
            $$ \begin{aligned}
            \frac{\partial z^{(l)}_p}{\partial \theta^{(l-1)}_{i,j}}
            &= \frac{\partial}{\partial \theta^{(l-1)}_{i,j}} \left( \sum_{k=1}^{n_{l-1}} \theta^{(l-1)}_{p,k} a^{(l-1)}_k \right) \\
            &= \sum_{k=1}^{n_{l-1}} \frac{\partial}{\partial \theta^{(l-1)}_{i,j}} \left( \theta^{(l-1)}_{p,k} a^{(l-1)}_k \right)
            \end{aligned} $$
            
            The result follows by noting that all terms in the summation, or all-but-one terms if $p = i$, are zero.
            
        \end{proof}
        
        
        
        
        \begin{theorem}
            
            $$
            \frac{\partial J}{\partial \theta^{(l-1)}_{i,j}} = \delta^{(l)}_i a^{(l-1)}_j
            $$
            
        \end{theorem}
        
        \begin{proof}
        
            Using the chain rule, the definition of $\boldsymbol{\delta}^{(l)}$ and the results from the previous lemmas,
            
            $$ \begin{aligned}
            \frac{\partial J}{\partial \theta^{(l-1)}_{i,j}}
            &= \sum_{k=1}^{n_l} \left( \frac{\partial J(\theta)}{\partial z^{(l)}_k} \ \frac{\partial z^{(l)}_k}{\partial \theta^{(l-1)}_{i,j}} \right) \\
            &= \sum_{k=1}^{n_l} \left( \delta^{(l)}_k \frac{\partial z^{(l)}_k}{\partial \theta^{(l-1)}_{i,j}} \right) \\
            &= \delta^{(l)}_i a^{(l-1)}_j
            \end{aligned}$$
        
        \end{proof}
        
        






\section{Demonstration of Methods}
\label{sec:demos}

    \subsection{Binary Classification}
    
    \subsection{Multiclass Classification}
        
        % TODO explain why we are only looking at net here, and not logreg
        Although logistic regression is limited to producing a binary output, 
        
        
        
        For the remainder of this section we will consider the Kuzushiji-MNIST (KMNIST) dataset \cite{kmnist}. 
        
        KMNIST is designed as a drop-in replacement for the infamous MNIST dataset of handwritten numeric digits, and therefore is identical in structure to MNIST: each observation is a single 28x28 matrix, where each element represents an "intensity" on a scale of 0 (white) to 1 (black), and therefore when visualised as a matrix plot



\section{Areas for Further Work}    



\bibliography{report}
\bibliographystyle{abbrv}

\end{document}
	
	