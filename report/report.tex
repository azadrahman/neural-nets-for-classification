\documentclass{article}[11pt]
\usepackage{arxiv}
\usepackage{url}

\usepackage{amssymb, amsmath}
\usepackage{amsthm}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\usepackage{algorithm}
\usepackage{algpseudocode}

\setlength{\jot}{12pt}

\usepackage{graphicx}
\graphicspath{graphics/}

\begin{document}
	
	
	\title{Neural Networks for Classification}
	\author{Owen Jones (olj23\@bath.ac.uk)}
	\date{Spring 2019}
	\maketitle


\begin{abstract}
    
    %TODO
    
	Here is the contents of my abstract blah blah blah blah blah.
\end{abstract}



\section{Introduction to Classification Problems}

    % TODO
    Historically, machine learning tasks have fallen into one of several categories: supervised learning problems, unsupervised learning problems, and more recently reinforcement learning
    
    Classification tasks (in the sense which this paper considers) belong to the supervised learning family of tasks, where the data used in the training process is \textit{labelled}
    
    qualitative outputs
    
    In Section \ref{sec:demos} of this paper, the 


\section{Logistic Regression}

    % TODO

    $y$ binary output (0 or 1)


    \subsection{Linear Regression}
        
        % TODO intro needs work
        The underlying assumption is always that the output occurs as a result of some combination of inputs.
        
        Mathematically speaking, suppose we have an observation $\mathbf{x}$ consisting of $n \geq 1$ inputs:
        
        $$
        \mathbf{x} = (x_1, \ x_2, \ \ldots, \ x_{n-1}, \ x_n) \in \mathbb{R}^n
        $$
        
        Then we assume that there exists a corresponding set of weights $\boldsymbol{\theta} = (\theta_0, \ldots, \theta_n) \in \mathbb{R}^{n+1}$ such that
        
        $$
        z = \theta_0 + \sum_{k=1}^n \theta_k x_k
        $$
        
        To simplify notation later on, we can augment $\mathbf{x}$ with $x_0 = 1$ allowing us to write $z$ as a single summation:
        
        $$
        z = \sum_{k=0}^n \theta_k x_k = \boldsymbol{\theta}^\top \mathbf{x}
        $$
        
        By adjusting the weights $\theta_0, \ldots, \theta_n$, different values of the output $z$ can be produced. Note that if we do not restrict $\boldsymbol{\theta}$ then $z$ is unbounded, i.e. $z \in (-\infty, \infty)$.
        
        
        
    \subsection{Introducing Nonlinearity}
        
        Recall that the goal of a binary classification task is to assign a categorical label to each observation; so if we wish to use linear regression as the basis for our algorithm, ultimately we will need to somehow interpret the continuous output $z \in \mathbb{R}$ as corresponding to one of those discrete labels.
        
        This can be achieved by means of a nonlinear \textit{activation function}:
        
        $$
        \sigma \colon (-\infty, \infty) \to (0, 1)
        $$
        
        In later sections we will consider other activation functions, but for logistic regression we typically use the \textit{sigmoid} function,
            
        $$
        \sigma_2(z) = \frac{1}{1 + e^{-z}}
        $$
        
        
        % TODO sigmoid plot
        
        
        Note how large positive values of $z$ are approximately mapped to 1, large negative values are approximately mapped to 0, and the transition between the two asymptotic limits is relatively sudden, yet smooth, with the central "threshold" at $z = 0$.
        
        In the remainder of this paper, the activated output $\sigma(z)$ is sometimes denoted as $a$ for the sake of notational simplicity.
        
        
        
    \subsection{The Cost Function}
    \label{sec:costfun}
        
        Now that we have a means of mapping an input $\mathbf{x}$ to an activated output $a \in (0, 1)$, we can begin to think about how to adjust the weights $\boldsymbol{\theta}$ so that $a$ is as close to the target output $y$ as possible.
        
        In order to assess how well the model is performing, we shall require some sort of "penalty value" based on the dissimilarity between $a$ and $y$. For example, the \textit{binary cross-entropy} is defined as follows:
        
        $$
        CE(\boldsymbol{\theta}) = \begin{cases}
            -\log(a) & y = 1 \\
            -\log(1-a) & y = 0
        \end{cases}
        $$
        
        
        % TODO plot
        
        
        Note that $CE$ is a function of $\boldsymbol{\theta}$ (since $a$ is a function of $z$, which is dependent on $\boldsymbol{\theta}$), that it takes a value of 0 if $a = y$, and that it increases exponentially as $a$ becomes further from $y$.
        
        So far we have considered a single observation $\mathbf{x}$ with a label $y$, which in combination with $\boldsymbol{\theta}$ produces a single activated output $a$. However in reality, we have multiple observations (say $m \geq 1$) each with an associated label and each producing an activated output in combination with $\boldsymbol{\theta}$. Therefore when assessing how well $\boldsymbol{\theta}$ parameterises our entire set of observations, we need to consider the cost of each of those observations.
        
        For that purpose, we generally consider some \textit{cost function} $J(\boldsymbol{\theta})$ which combines the individual costs from each observation. Such a function effectively quantifies how "wrong" the model is, so when adjusting $\boldsymbol{\theta}$ we will be aiming to minimise $J(\boldsymbol{\theta})$. A typical choice for binary classification problems is the \textit{binary cross-entropy cost function}:
        
        $$
        J_2(\boldsymbol{\theta}) = \frac{1}{m} \sum_{i=1}^{m} CE(\boldsymbol{\theta}) = - \frac{1}{m} \sum_{i=1}^{m} \left( y_i \log(a_i) + (1 - y_i) \log(1 - a_i) \right)
        $$
        
        Since $y_i \in \{0, 1\}$, at most one of the terms in the body of the summation is non-zero for each $i$; and so $J_2(\boldsymbol{\theta})$ is simply the average cross-entropy across all $m$ observations in our dataset.
    
    
    
    \subsection{Regularisation}
        
        % TODO
        
        
    \subsection{Training}
        
        As described at the end of Section \ref{sec:costfun}, the goal of the \textit{training} process is to minimise the cost function $J(\boldsymbol{\theta})$ by making changes to the parameters $\boldsymbol{\theta}$: in other words, we essentially have an optimisation problem, which we can tackle in countless different ways.
        
        Here we will focus on \textit{gradient descent}, an iterative method which takes successive steps converging towards a minimum optimal value. Although many other more sophisticated optimisation algorithms exist, gradient descent is one of the most commonly used optimisation methods due to its simplicity yet relative effectiveness: see \cite{lecun_backprop} for further details.
        
        
        \subsubsection{Descent Direction}
            
            Since we wish to reduce $J(\boldsymbol{\theta})$ by taking some step in parameter space, before doing anything else we must determine the direction in which to take that step.
            
            Consider a perturbation $\Delta \boldsymbol{\theta}$ to our current parameters $\boldsymbol{\theta} \in \mathbb{R}^{n+1}$, and notice that a Taylor expansion yields
            
            $$
            J(\boldsymbol{\theta} + \Delta \boldsymbol{\theta}) = J(\boldsymbol{\theta}) + \sum_{k=0}^{n+1} \frac{\partial J(\boldsymbol{\theta})}{\partial \theta_k} \Delta \boldsymbol{\theta} + \mathcal{O} ((\Delta \boldsymbol{\theta})^2)
            $$
            
            Then ignoring higher-order terms, we have
            
            $$
            J(\boldsymbol{\theta} + \Delta \boldsymbol{\theta}) \approx J(\boldsymbol{\theta}) + \nabla J(\boldsymbol{\theta})^\top \Delta \boldsymbol{\theta}
            $$
            
            Now we greedily choose $\Delta \boldsymbol{\theta}$ such that we reduce $J(\boldsymbol{\theta})$ by as much as possible - i.e. by making $\nabla J(\boldsymbol{\theta})^\top \Delta \boldsymbol{\theta}$ as negative as possible. By the Cauchy-Schwarz inequality, the most negative value of $\mathbf{f}^\top \mathbf{g}$ for any $\mathbf{f}, \mathbf{g} \in \mathbb{R}^{n+1}$ is $- \lVert \mathbf{f} \rVert_2 \lVert \mathbf{g} \rVert_2$, when $\mathbf{f} = -\mathbf{g}$. Therefore we shall choose $\Delta \boldsymbol{\theta} = - \nabla J(\boldsymbol{\theta})$ as our \textit{descent direction}. Note that $\nabla J(\boldsymbol{\theta})^\top (- \nabla J(\boldsymbol{\theta})) = -\lVert \nabla J(\boldsymbol{\theta}) \rVert_2^2 \leq 0$ with equality if and only if $\nabla J(\boldsymbol{\theta}) = 0$, and consequently $J(\boldsymbol{\theta} + \Delta \boldsymbol{\theta}) < J(\boldsymbol{\theta})$ so long as $\nabla J(\boldsymbol{\theta}) \neq 0$.
        
    
    
        \subsubsection{Learning rate}
            
            While it is guaranteed that the "steepest descent" direction we have just chosen, $\Delta \boldsymbol{\theta} = - \nabla J(\boldsymbol{\theta})$, is indeed a descent direction, we must recall at this point that we are using a Taylor series approximation for $J(\boldsymbol{\theta} + \Delta \boldsymbol{\theta})$. Therefore this guarantee only applies in some (potentially very small) radius of the parameter space, centred on $\boldsymbol{\theta}$.
            
            Consequently in order to ensure that we obtain a reduction in $J$, we must scale $\Delta \boldsymbol{\theta}$ by some value $\eta$. This value is called the \textit{learning rate}.
            
            Selecting a learning rate is often a delicate task. Too small a value results in undersized steps in the descent direction, meaning convergence might be slow; too large a value may result in "overshooting" the region where descent is guaranteed, leading to oscillation around the optimum or even to complete divergence.
            
            As such, it is often favourable to use an \textit{adaptive learning rate}: rather than a fixed constant $\eta$, a new value $\eta_k$ is used at each iteration of the algorithm. Many methods exist for determining such adaptive rates, often based on line search methods - see \cite{line_search} and \cite{adam}.
            
            In this paper we will use a relatively simple method based on backtracking line search, where the learning rate is steadily grown by some constant factor at each iteration, and then more dramatically decreased when it is too large.
            
            
            \begin{algorithm} \label{alg:grow_and_slash}
                
                \caption{"Grow and slash" learning rate}
                
                \begin{algorithmic}
                
                    \State Choose $\eta_1 > 0$, $\alpha > 1$, $\beta > 1$ (in practice, $\alpha = 1.1$, $\beta = 2$ work well).
                    
                    \For {iterations $k = 1, ...$ of gradient descent}
                        \While {$J(\boldsymbol{\theta} + \eta_k (\Delta \boldsymbol{\theta})) > J(\boldsymbol{\theta})$}
                            \State $\eta_k \gets \eta_k / \beta$
                        \EndWhile
                        
                        \State $\eta_{k+1} \gets \alpha \eta_k$
                    \EndFor
                    
                \end{algorithmic}
            
            \end{algorithm}
                    
                
            
            
            % TODO summarise
        



\section{Neural Networks}
    
    % TODO
    Blah blah blah.
    
    % TODO Mention single-neuron non-activated output for regression?
    
    

    \subsection{Multiple Outputs}
    
        So far we have considered passing the input $\mathbf{x}$ into a single logistic regression model, yielding a single scalar output $a$.
    
        By passing the same input $\mathbf{x}$ to $n$ separate logistic regression models, we can obtain $n$ separate outputs $a_1, \ldots, a_n$. If each model is initialised with a different set of weights $\boldsymbol{\theta_n}$, each would produce a slightly different output.
        
        Now suppose that we have a classification problem where each observation can belong to one of $n \geq 2$ classes. We can characterise the target output as a \textit{one-hot encoded} vector $\mathbf{y} \in \mathbb{R}^n$: for $\mathbf{x}$ belonging to class $k$, we define
        
        $$
        y_k = \begin{cases}
            1 & \text{observation belongs to class } k \\
            0 & \text{otherwise}
        \end{cases}
        $$
        
        Thus we can train the $k$th model to produce an output of 1, and the other $(n-1)$ models to produce an output of 0; and consequently we can obtain a prediction for the class of a new observation by seeing which model produces the output closest to 1.

        Moreover, often in such settings it is useful to have not only a prediction of the class, but also some estimation of the \textit{certainty} of that prediction.
        
        This motivates the introduction of a new activation function, the \textit{softmax} function: if $\mathbf{z} \in \mathbb{R}^n$, then for $j = 1, \ldots, n$,
        
        $$
        \sigma(z_j) = \frac{e^{z_j}}{\sum_{k=1}^{n} e^{z_k}}
        $$
        
        Since this is a normalisation of the exponentiated raw output $\mathbf{z}$, the elements of the resulting vector $\mathbf{a}$ sum to 1; and therefore each of these $n$ values can be interpreted as the probability of $x$ belonging to that class.
        
        Note the following useful property of the softmax function: 
        
        \begin{proposition}
            
            The softmax function is invariant to constant shifts.
            
        \end{proposition}
    
        \begin{proof}
            
            Let $c \in \mathbb{R}$, and let $\mathbf{\bar{z}} \in \mathbb{R}^n$ be defined by $\bar{z}_j = z_j - c$ for all $j = 1, \ldots, n$. Observe that
            
            $$ \begin{aligned}
                \sigma(\bar{z}_j)
                &= \frac{e^{\bar{z}_j}}{\sum_{k=1}^{n} e^{\bar{z}_k}}
                &= \frac{e^{z_j - c}}{\sum_{k=1}^{n} e^{z_k - c}} \\
                &= \frac{e^{-c} e^{z_j}}{\sum_{k=1}^{n} e^{-c} e^{z_k}} \\
                &= \frac{e^{z_j}}{\sum_{k=1}^{n} e^{z_k}} \\
                &= \sigma(z_j)
            \end{aligned} $$
            
        \end{proof}
        
        In practical applications, a shift of $c = \max_{j = 1, \ldots, n} z_j$ is often applied since this eliminates the slight risk of computational inaccuracies being introduced by very large values of $e^{z_j}$.
        
        The \textit{multiclass cross-entropy cost function} is typically used with multiclass softmax outputs:
        
        $$
        J(\boldsymbol{\theta}) = \frac{1}{m} \sum_{i=1}^{m} \left( - \sum_{k=1}^{n} (y_i)_k \log((a_i)_k) \right)
        $$
        
        Note that this is in fact a generalisation of the binary cross-entropy cost function: since $\mathbf{y_i}$ is a one-hot vector, only one term of the inner summation is non-zero, just as only one of the two terms in the binary cross-entropy is non-zero.
        
        
        
    
    \subsection{Layers and Forward Propagation}
        
        Consider 
        
        
        
        
        
        
        
        
        
        Since we now have a set of outputs $\mathbf{z}$ (and activated outputs $\mathbf{a}$) from each layer of our network, some extensions to the notation we have been using so far are necessary: we shall henceforth use a superscript index to indicate the layer which produces each output, i.e. in an $L$-layer network we will have $\mathbf{z}^{(l)}$ and $\mathbf{a}^{(l)}$ for $l = 1, \ldots, L$. Moreover we shall denote the dimension of these vectors as $n_l$ for each $l$, where a bias unit is implicitly included as the first element of each input. % TODO is this okay??
        
        

    \subsection{Backpropagation}
        
        
        $$ \begin{aligned}
        J(\theta)
        &= - \sum_{k=1}^{n} \left( y_k \log(a^{(L)}_k) + (1 - y_k) \log(1 - a^{(L)}_k) \right) \\
        &= -\mathbf{y}^\top \log(\mathbf{a}^{(L)}) - (\mathbf{1}-\mathbf{y})^\top \log(\mathbf{1} - \mathbf{a}^{(L)})
        \end{aligned} $$
        
        
        
        $$
        \frac{\partial J}{\partial \theta^{(L-1)}} =
            \frac{\partial J}{\partial \mathbf{a}^{(L)}} \
            \frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{z}^{(L)}} \
            \frac{\partial \mathbf{z}^{(L)}}{\partial \theta^{(L-1)}}
        $$



        $$
        \frac{\partial J}{\partial \theta^{(L-2)}} =
            \frac{\partial J}{\partial \mathbf{a}^{(L)}} \
            \frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{z}^{(L)}} \
            \frac{\partial \mathbf{z}^{(L)}}{\partial \mathbf{a}^{(L-1)}} \
            \frac{\partial \mathbf{a}^{(L-1)}}{\partial \mathbf{z}^{(L-1)}} \
            \frac{\partial \mathbf{z}^{(L-1)}}{\partial \mathbf{a}^{(L-2)}}
        $$



        Where we define the "error" of layer $l$ as
    
        $$
        \mathbf{\delta}^{(l)} :=  \frac{\partial J}{\partial \mathbf{z}^{(l)}} \in \mathbb{R}^{n_l}
        $$
        
        The $j$th component of this error vector is a measure of the sensitivity of the cost function $J(\theta)$ to the (weighted) input from neuron $j$ in layer $l$. Although it is \textit{not} a direct measure of how much a particular neuron is at fault for mistakes in the network's eventual output, such an explanation is at least valid intuitively, since all partial derivatives must be zero if the cost function is at a minimum.
        
        
        % TODO wording!!!
        Now define recursively... for $l = 1, \ldots, L-1$
        
        $$
        \delta^{(l)} =
        \delta^{(l+1)} \
        \frac{\partial \mathbf{z}^{(l+1)}}{\partial \mathbf{a}^{(l)}} \
        \frac{\partial \mathbf{a}^{(l)}}{\partial \mathbf{z}^{(l)}} \
        $$
        
        
        
        % TODO reword this - binary/multiclass?
        Note that when $J(\theta)$ is the cross-entropy cost function, the error in the final layer $L$ can be calculated as follows:
        
        
        
        
        In all the following results, we consider the cost for a dataset of size $m = 1$; but note that everything immediately generalises to larger values of $m$, since in all cases differentiation distributes over the outer summation (over $m$).
        
        
        
        \begin{lemma}
            
            If $J_2$ is the binomial cross-entropy cost function, and $\sigma_2$ is the sigmoid activation function, then for all $j = 1, \ \ldots, \ n_L$
            
            $$
            \delta^{(L)}_j = a^{(L)}_j - y_j
            $$
            
        \end{lemma}
        
        
        \begin{proof}
            
            Observe that the partial derivative of $J_2$ with respect to $a^{(L)}_i$ (the $i$th component of the activated output $\mathbf{a}^{(L)}$) can be represented as
                    
            $$ \begin{aligned}
            \frac{\partial J_2}{\partial a^{(L)}_i}
            &= \frac{\partial}{\partial a^{(L)}_i} \left( - y_i \log(a^{(L)}_i) - (1 - y_i) \log(1 - a^{(L)}_i) \right) \\
            &= - \frac{y_i}{a^{(L)}_i} + \frac{1 - y_i}{1 - a^{(L)}_i}
            \end{aligned} $$
            
            Consider now the partial derivative of the activated output of layer $L$, with respect to the non-activated output; that is,
            
            $$ \begin{aligned}
            \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j}
            &= \frac{\partial}{\partial z^{(L)}_j} \left( \sigma_2(z^{(L)}_j) \right)
            &= \frac{\partial}{\partial z^{(L)}_j} \left( \frac{1}{1 + e^{-z^{(L)}_i}} \right)
            \end{aligned} $$
                
            Note that if $i \neq j$ then this is clearly zero. If $i = j$,
            
            $$ \begin{aligned}
            \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j}
            &= \left( - e^{-z^{(L)}_j} \right) \left( - \left( 1 + e^{-z^{(L)}_j} \right)^{-2} \right) \\
            &= \frac{e^{-z^{(L)}_j}}{\left( 1 + e^{-z^{(L)}_j} \right) \left( 1 + e^{-z^{(L)}_j} \right)} \\
            &= \left( \frac{1}{1 + e^{-z^{(L)}_j}} \right) \left( \frac{\left( 1 + e^{-z^{(L)}_j} \right) - 1}{1 + e^{-z^{(L)}_j}} \right) \\
            &= \sigma_2(z^{(L)}_j) \left( 1 - \sigma_2(z^{(L)}_j) \right) \\
            &= a^{(L)}_j \left( 1 - a^{(L)}_j \right)
            \end{aligned} $$
            
            Now using the definition of $\boldsymbol{\delta}^{(L)}$ and the chain rule,
            
            $$ \begin{aligned}
            \delta^{(L)}_j &= \frac{\partial J_2(\theta)}{\partial z^{(L)}_j} \\
            &= \sum_{i=1}^{n_L} \left( \frac{\partial J(\theta)}{\partial a^{(L)}_i} \ \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j} \right) \\
            &= \frac{\partial J(\theta)}{\partial a^{(L)}_j} \ \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j} + \sum_{\substack{i=1 \\ i \neq j}}^{n_L} \left( \frac{\partial J(\theta)}{\partial a^{(L)}_i} \ \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j} \right) \\
            &= \frac{\partial J(\theta)}{\partial a^{(L)}_j} \ \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j} + 0 \\
            &= \left( - \frac{y_j}{a^{(L)}_j} + \frac{1 - y_j}{1 - a^{(L)}_j} \right) a^{(L)}_j \left( 1 - a^{(L)}_j \right) \\
            &= a^{(L)}_j (1 - y_j) - y_j (1 - a^{(L)}_j) \\
            &= a^{(L)}_j - y_j
            \end{aligned} $$
            
        \end{proof}
        
        
        % TODO word nicely
        We now complete a similar proof for the softmax case.
        
        
        \begin{lemma}
            
            If $J$ is the multiclass cross-entropy cost function, and $\sigma$ is the softmax activation function, then for all $j = 1, \ \ldots, \ n_L$
            
            $$
            \delta^{(L)}_j = a^{(L)}_j - y_j
            $$
            
        \end{lemma}
        
        
        \begin{proof}
            
            Firstly note that
            
            $$ \begin{aligned}
            \frac{\partial J}{\partial a^{(L)}_i}
            &= \frac{\partial}{\partial a^{(L)}_i} \left( - \sum_{k=1}^{n_L} y_k \log(a^{(L)}_k) \right) \\
            &= - \frac{y_i}{a^{(L)}_i}
            \end{aligned} $$
            
            
            Then using the quotient rule,
            
            $$ \begin{aligned}
            \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j}
            &= \frac{\partial}{\partial z^{(L)}_j} \left( \frac{e^{z^{(L)}_i}}{\sum_{k=1}^{n_L} e^{z^{(L)}_k}} \right) \\
            &= \frac{\left( \frac{\partial \left( e^{z^{(L)}_i}\right)}{\partial z^{(L)}_j} \cdot \sum_{k=1}^{n_L} e^{z^{(L)}_k} \right) - e^{z^{(L)}_i} e^{z^{(L)}_j}}{\left( \sum_{k=1}^{n_L} e^{z^{(L)}_k} \right)^2}
            \end{aligned} $$
            
            \begin{itemize}
            
                \item If $i \neq j$,
            
                $$ \begin{aligned}
                \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j}
                &= \frac{0 - e^{z_i} e^{z_j}}{\left( \sum_{k=1}^{n_L} e^{z^{(L)}_k} \right)^2} \\
                &= - \sigma(z_i) \sigma(z_j) \\
                &= - a^{(L)}_i a^{(L)}_j
                \end{aligned} $$
            
                \item On the other hand, if $i = j$,
            
                $$ \begin{aligned}
                \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j}
                &= \frac{\left( e^{z^{(L)}_j} \cdot \sum_{k=1}^{n_L} e^{z^{(L)}_k} \right) - \left( e^{z^{(L)}_j} \right)^2}{\left( \sum_{k=1}^{n_L} e^{z^{(L)}_k} \right)^2} \\
                &= \frac{e^{z^{(L)}_j}}{\sum_{k=1}^{n_L} e^{z^{(L)}_k}} \left( 1 - \frac{e^{z^{(L)}_j}}{\sum_{k=1}^{n_L} e^{z^{(L)}_k}} \right) \\
                &= \sigma(z^{(L)}_j) \left( 1 - \sigma(z^{(L)}_j) \right) \\
                &= a^{(L)}_j \left( 1 - a^{(L)}_j \right)
                \end{aligned} $$
            
            \end{itemize}
            
            Now once again using the definition of $\boldsymbol{\delta}^{(L)}$ and the chain rule, observe that
            
            $$ \begin{aligned}
            \delta^{(L)}_j &= \frac{\partial J(\theta)}{\partial z^{(L)}_j} \\
            &= \sum_{i=1}^{n_L} \left( \frac{\partial J(\theta)}{\partial a^{(L)}_i} \ \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j} \right) \\
            &= \frac{\partial J(\theta)}{\partial a^{(L)}_j} \ \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j} + \sum_{\substack{i=1 \\ i \neq j}}^{n_L} \left( \frac{\partial J(\theta)}{\partial a^{(L)}_i} \ \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j} \right) \\
            &= - \frac{y_j}{a^{(L)}_j} \left( a^{(L)}_j (1 - a^{(L)}_j) \right) + \sum_{\substack{i=1 \\ i \neq j}}^{n_L} \left( - \frac{y_i}{a^{(L)}_i} \left(- a^{(L)}_i a^{(L)}_j \right) \right) \\
            &= - y_j (1 - a^{(L)}_j) + a^{(L)}_j \left( \sum_{i=1}^{n_L} \left( y_i \right) - y_j \right)
            \end{aligned} $$
            
            This looks slightly complicated, but recall that $\mathbf{y}$ is a one-hot encoded vector, and therefore $\sum_{=1}^{n_L} y_i = 1$. Hence we have
            
            $$ \begin{aligned}
            \delta^{(L)}_j &= - y_j (1 - a^{(L)}_j) + a^{(L)}_j (1 - y_j) \\
            &= a^{(L)}_j - y_j
            \end{aligned} $$
            
        \end{proof}
    
        
        
        \begin{lemma}
            
            For all $i = 1, \ \ldots, \ n_l$ and all $j = 1, \ \ldots, \ n_{l-1}$,
            
            $$
            \frac{\partial z^{(l)}_i}{\partial a^{(l-1)}_j} = \theta^{(l-1)}_{i,j}
            $$
            
        \end{lemma}
        
        \begin{proof}
            
            By the definition of $z^{(l)}_i$, we have
            
            $$ \begin{aligned}
            \frac{\partial z^{(l)}_i}{\partial a^{(l-1)}_j}
            &= \frac{\partial}{\partial a^{(l-1)}_j} \left( \sum_{k = 1}^{n_{l-1}} a^{(l-1)}_k \theta^{(l-1)}_{i,k} \right) \\
            &= \theta^{(l-1)}_{i,j}
            \end{aligned} $$
            
        \end{proof}
        
        
        
        
        \begin{lemma}
            
            For all $i = 1, \ \ldots, \ n_l$ and all $j = 1, \ \ldots, \ n_{l-1}$,
            
            $$
            \frac{\partial z^{(l)}_p}{\partial \theta^{(l-1)}_{i,j}} = \begin{cases}
                0 & p \neq i \\
                a^{(l-1)}_j & p = i
            \end{cases}
            $$
            
        \end{lemma}
        
        \begin{proof}
            
            Using the definition of $z^{(l)}_p$,
            
            $$ \begin{aligned}
            \frac{\partial z^{(l)}_p}{\partial \theta^{(l-1)}_{i,j}}
            &= \frac{\partial}{\partial \theta^{(l-1)}_{i,j}} \left( \sum_{k=1}^{n_{l-1}} \theta^{(l-1)}_{p,k} a^{(l-1)}_k \right) \\
            &= \sum_{k=1}^{n_{l-1}} \frac{\partial}{\partial \theta^{(l-1)}_{i,j}} \left( \theta^{(l-1)}_{p,k} a^{(l-1)}_k \right)
            \end{aligned} $$
            
            The result follows by noting that all terms in the summation, or all-but-one terms if $p = i$, are zero.
            
        \end{proof}
        
        
        
        
        \begin{theorem}
            
            $$
            \frac{\partial J}{\partial \theta^{(l-1)}_{i,j}} = \delta^{(l)}_i a^{(l-1)}_j
            $$
            
        \end{theorem}
        
        \begin{proof}
        
            Using the chain rule, the definition of $\boldsymbol{\delta}^{(l)}$ and the results from the previous lemmas,
            
            $$ \begin{aligned}
            \frac{\partial J}{\partial \theta^{(l-1)}_{i,j}}
            &= \sum_{k=1}^{n_l} \left( \frac{\partial J(\theta)}{\partial z^{(l)}_k} \ \frac{\partial z^{(l)}_k}{\partial \theta^{(l-1)}_{i,j}} \right) \\
            &= \sum_{k=1}^{n_l} \left( \delta^{(l)}_k \frac{\partial z^{(l)}_k}{\partial \theta^{(l-1)}_{i,j}} \right) \\
            &= \delta^{(l)}_i a^{(l-1)}_j
            \end{aligned}$$
        
        \end{proof}
        
        



    \subsection{Improvements to Training}
        
        
        \subsubsection{Stochastic Gradient Descent}
        
        \subsubsection{Validation and Testing}



        \subsubsection{Early Stopping}




\section{Demonstration of Methods}
\label{sec:demos}

    \subsection{Binary Classification}
    
    \subsection{Multiclass Classification}
        
        % TODO explain why we are only looking at net here, and not logreg
        Although logistic regression is limited to producing a binary output, 
        
        
        
        For the remainder of this section we will consider the Kuzushiji-MNIST (KMNIST) dataset \cite{kmnist}. 
        
        KMNIST is designed as a drop-in replacement for the infamous MNIST dataset of handwritten numeric digits, and therefore is identical in structure to MNIST: each observation is a single 28x28 matrix, where each element represents an "intensity" on a scale of 0 (white) to 1 (black), and therefore when visualised as a matrix plot



\section{Areas for Further Work}



\bibliography{report}
\bibliographystyle{abbrv}

\end{document}
	