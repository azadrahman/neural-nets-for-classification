\documentclass{article}[11pt]
\usepackage{arxiv}
\usepackage{url}

\usepackage{amssymb, amsmath}
\usepackage{amsthm}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\setlength{\jot}{12pt}

\usepackage{graphicx}
\graphicspath{graphics/}

\begin{document}
	
	
	\title{Neural Networks for Classification}
	\author{Owen Jones (olj23\@bath.ac.uk)}
	\date{Spring 2019}
	\maketitle


\begin{abstract}
	Here is the contents of my abstract blah blah blah blah blah.
\end{abstract}



\section{Introduction to Classification Problems}

    Historically, machine learning tasks have fallen into one of several categories: supervised learning problems, unsupervised learning problems, and more recently reinforcement learning
    
    Classification tasks (in the sense which this paper considers) belong to the supervised learning family of tasks, where the training process uses
    
    qualitative outputs
    
    In Section \ref{sec:demos} of this paper, the 


\section{Logistic Regression}

    

    \subsection{Introducing Nonlinearity}

    \subsection{The Cost Function}

    \subsection{Regularisation}
    
    \subsection{Training}
    
    % TODO
    Hyperparams
    Variable learning rate
    Validation set
    Early stopping



\section{Neural Networks}

    \subsection{Multiple Outputs}
    
    \subsection{Forward Propagation}

    \subsection{Backpropagation}
        
        
        $$ \begin{aligned}
        J(\theta)
        &= - \sum_{k=0}^{n} \left( y_k \log(a^{(L)}_k) + (1 - y_k) \log(1 - a^{(L)}_k) \right) \\
        &= -\mathbf{y}^\top \log(\mathbf{a}^{(L)}) - (\mathbf{1}-\mathbf{y})^\top \log(\mathbf{1} - \mathbf{a}^{(L)})
        \end{aligned} $$
        
        
        
        $$
        \frac{\partial J}{\partial \theta^{(L-1)}} =
            \frac{\partial J}{\partial \mathbf{a}^{(L)}} \
            \frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{z}^{(L)}} \
            \frac{\partial \mathbf{z}^{(L)}}{\partial \theta^{(L-1)}}
        $$



        $$
        \frac{\partial J}{\partial \theta^{(L-2)}} =
            \frac{\partial J}{\partial \mathbf{a}^{(L)}} \
            \frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{z}^{(L)}} \
            \frac{\partial \mathbf{z}^{(L)}}{\partial \mathbf{a}^{(L-1)}} \
            \frac{\partial \mathbf{a}^{(L-1)}}{\partial \mathbf{z}^{(L-1)}} \
            \frac{\partial \mathbf{z}^{(L-1)}}{\partial \mathbf{a}^{(L-2)}}
        $$



        Where we define the "error" of layer $l$ as
    
        $$
        \mathbf{\delta}^{(l)} :=  \frac{\partial J}{\partial \mathbf{z}^{(l)}} \in \mathbb{R}^{n_l}
        $$
        
        The $j$th component of this error vector is a measure of the sensitivity of the cost function $J(\theta)$ to the (weighted) input from neuron $j$ in layer $l$. Although it is \textit{not} a direct measure of how much a particular neuron is at fault for mistakes in the network's eventual output, such an explanation is at least valid intuitively, since all partial derivatives must be zero if the cost function is at a minimum.
        
        % TODO reword this - binary/multiclass?
        Note that when $J(\theta)$ is the cross-entropy cost function, the error in the final layer $L$ can be calculated as follows:
        
        \begin{lemma}
            
            If $J$ is the \textbf{binomial cross-entropy} cost function, and $\sigma$ is the \textbf{sigmoid} activation function, then
            
            $$
            \boldsymbol{\delta}^{(L)} = \mathbf{a}^{(L)} - \mathbf{y}
            $$
            
        \end{lemma}
        
        
        \begin{proof}
            
            Denoting the expected output of the network as $\mathbf{y}$, observe that the partial derivative of $J$ with respect to the $j$th component of the activated output $\mathbf{a}^{(L)}$ can be represented as
                    
            $$ \begin{aligned}
            \frac{\partial J(\theta)}{\partial a^{(L)}_i}
            &= \frac{\partial}{\partial a^{(L)}_i} \left( - \sum_{k=0}^{n_L} \left( y_k \log(a^{(L)}_k) + (1 - y_k) \log(1 - a^{(L)}_k) \right) \right) \\
            &= - \frac{y_i}{a^{(L)}_i} + \frac{1 - y_i}{1 - a^{(L)}_i}
            \end{aligned} $$
            
            Consider now the partial derivative of the activated output of layer $L$, with respect to the non-activated output; that is,
            
            $$ \begin{aligned}
            \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j}
            &= \frac{\partial}{\partial z^{(L)}_j} \left( \sigma(z^{(L)}_j) \right)
            &= \frac{\partial}{\partial z^{(L)}_j} \left( \frac{1}{1 + e^{-z^{(L)}_i}} \right)
            \end{aligned} $$
                
            Note that if $i \neq j$ then this is clearly zero. If $i = j$,
            
            $$ \begin{aligned}
            \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j}
            &= \left( - e^{-z^{(L)}_j} \right) \left( - \left( 1 + e^{-z^{(L)}_j} \right)^{-2} \right) \\
            &= \frac{e^{-z^{(L)}_j}}{\left( 1 + e^{-z^{(L)}_j} \right) \left( 1 + e^{-z^{(L)}_j} \right)} \\
            &= \left( \frac{1}{1 + e^{-z^{(L)}_j}} \right) \left( \frac{\left( 1 + e^{-z^{(L)}_j} \right) - 1}{1 + e^{-z^{(L)}_j}} \right) \\
            &= \sigma(z^{(L)}_j) \left( 1 - \sigma(z^{(L)}_j) \right) \\
            &= a^{(L)}_j \left( 1 - a^{(L)}_j \right)
            \end{aligned} $$
            
            Now using the definition of $\boldsymbol{\delta}^{(L)}$ and the chain rule,
            
            $$ \begin{aligned}
            \delta^{(L)}_j &= \frac{\partial J(\theta)}{\partial z^{(L)}_j} \\
            &= \sum_{i = 0}^{n_L} \left( \frac{\partial J(\theta)}{\partial a^{(L)}_i} \ \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j} \right) \\
            &= \frac{\partial J(\theta)}{\partial a^{(L)}_j} \ \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j} + \sum_{\substack{i = 0 \\ i \neq j}}^{n_L} \left( \frac{\partial J(\theta)}{\partial a^{(L)}_i} \ \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j} \right) \\
            &= \frac{\partial J(\theta)}{\partial a^{(L)}_j} \ \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j} + 0 \\
            &= \left( - \frac{y_j}{a^{(L)}_j} + \frac{1 - y_j}{1 - a^{(L)}_j} \right) a^{(L)}_j \left( 1 - a^{(L)}_j \right) \\
            &= a^{(L)}_j (1 - y_j) - y_j (1 - a^{(L)}_j) \\
            &= a^{(L)}_j - y_j
            \end{aligned} $$
            
        \end{proof}
        
        
        % TODO word nicely
        We now complete a similar proof for the softmax case.
        
        
        \begin{lemma}
            
            If $J$ is the \textbf{multiclass cross-entropy} cost function, and $\sigma$ is the \textbf{softmax} activation function, then
            
            $$
            \boldsymbol{\delta}^{(L)} = \mathbf{a}^{(L)} - \mathbf{y}
            $$
            
        \end{lemma}
        
        
        \begin{proof}
            
            Firstly note that
            
            $$ \begin{aligned}
            \frac{\partial J(\theta)}{\partial a^{(L)}_i}
            &= \frac{\partial}{\partial a^{(L)}_i} \left( - \sum_{k=0}^{n_L} y_k \log(a^{(L)}_k) \right) \\
            &= - \frac{y_i}{a^{(L)}_i}
            \end{aligned} $$
            
            
            Then using the quotient rule,
            
            $$ \begin{aligned}
            \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j}
            &= \frac{\partial}{\partial z^{(L)}_j} \left( \frac{e^{z^{(L)}_i}}{\sum_{k=0}^{n_L} e^{z^{(L)}_k}} \right) \\
            &= \frac{\left( \frac{\partial \left( e^{z^{(L)}_i}\right)}{\partial z^{(L)}_j} \cdot \sum_{k=0}^{n_L} e^{z^{(L)}_k} \right) - e^{z^{(L)}_i} e^{z^{(L)}_j}}{\left( \sum_{k=0}^{n_L} e^{z^{(L)}_k} \right)^2}
            \end{aligned} $$
            
            \begin{itemize}
            
                \item If $i \neq j$,
            
                $$ \begin{aligned}
                \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j}
                &= \frac{0 - e^{z_i} e^{z_j}}{\left( \sum_{k=0}^{n_L} e^{z^{(L)}_k} \right)^2} \\
                &= - \sigma(z_i) \sigma(z_j) \\
                &= - a^{(L)}_i a^{(L)}_j
                \end{aligned} $$
            
                \item On the other hand, if $i = j$,
            
                $$ \begin{aligned}
                \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j}
                &= \frac{\left( e^{z^{(L)}_j} \cdot \sum_{k=0}^{n_L} e^{z^{(L)}_k} \right) - \left( e^{z^{(L)}_j} \right)^2}{\left( \sum_{k=0}^{n_L} e^{z^{(L)}_k} \right)^2} \\
                &= \frac{e^{z^{(L)}_j}}{\sum_{k=0}^{n_L} e^{z^{(L)}_k}} \left( 1 - \frac{e^{z^{(L)}_j}}{\sum_{k=0}^{n_L} e^{z^{(L)}_k}} \right) \\
                &= \sigma(z^{(L)}_j) \left( 1 - \sigma(z^{(L)}_j) \right) \\
                &= a^{(L)}_j \left( 1 - a^{(L)}_j \right)
                \end{aligned} $$
            
            \end{itemize}
            
            Then once again using the definition of $\boldsymbol{\delta}^{(L)}$ and the chain rule, observe that
            
            $$ \begin{aligned}
            \delta^{(L)}_j &= \frac{\partial J(\theta)}{\partial z^{(L)}_j} \\
            &= \sum_{i = 0}^{n_L} \left( \frac{\partial J(\theta)}{\partial a^{(L)}_i} \ \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j} \right) \\
            &= \frac{\partial J(\theta)}{\partial a^{(L)}_j} \ \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j} + \sum_{\substack{i = 0 \\ i \neq j}}^{n_L} \left( \frac{\partial J(\theta)}{\partial a^{(L)}_i} \ \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j} \right) \\
            &= - \frac{y_j}{a^{(L)}_j} \left( a^{(L)}_j (1 - a^{(L)}_j) \right) + \sum_{\substack{i = 0 \\ i \neq j}}^{n_L} \left( - \frac{y_i}{a^{(L)}_i} \left(- a^{(L)}_i a^{(L)}_j \right) \right) \\
            &= - y_j (1 - a^{(L)}_j) + a^{(L)}_j \left( \sum_{i = 0}^{n_L} \left( y_i \right) - y_j \right)
            \end{aligned} $$
            
            This looks slightly complicated, but recall that $\mathbf{y}$ is a one-hot encoded vector, and therefore $\sum_{i = 0}^{n_L} y_i = 1$. Hence we have
            
            $$ \begin{aligned}
            \delta^{(L)}_j &= - y_j (1 - a^{(L)}_j) + a^{(L)}_j (1 - y_j) \\
            &= a^{(L)}_j - y_j
            \end{aligned} $$
            
        \end{proof}
        
        
        % TODO wording!!!
        Now define recursively... for $l = 0, \ldots, L-1$
        
        $$
        \delta^{(l)} =
        \delta^{(l+1)} \
        \frac{\partial \mathbf{z}^{(l+1)}}{\partial \mathbf{a}^{(l)}} \
        \frac{\partial \mathbf{a}^{(l)}}{\partial \mathbf{z}^{(l)}} \
        $$
        
        
        \begin{lemma}
            
            $$
            \frac{\partial \mathbf{a}^{(l)}}{\partial \mathbf{z}^{(l)}} = \textnormal{diag} \left( \sigma'(\mathbf{z}^{(l)}) \right) = \left( \begin{array}{ccc}
            \sigma'(z^{(l)}_0) & & 0 \\
            & \ddots &  \\
            0 & & \sigma'(z^{(l)}_{n_l}) \\
            \end{array} \right)
            $$
            
        \end{lemma}
    
        \begin{proof}
        
            Since $a^{(l)}_i = \sigma(z^{(l)}_i)$, we have
            
            $$
            \frac{\partial a^{(l)}_i}{\partial z^{(l)}_j} = \frac{\partial \sigma(z^{(l)}_i)}{\partial z^{(l)}_j} = \begin{cases}
                0 & i \neq j \\
                \sigma'(z^{(l)}_j) & i = j
            \end{cases}
            $$
        
        \end{proof}
        
        
        
        \begin{lemma}
            
            $$
            \frac{\partial \mathbf{z}^{(l)}}{\partial \mathbf{a}^{(l-1)}} = \theta^{(l-1)}
            $$
            
        \end{lemma}
        
        \begin{proof}
            
            By the definition of $z^{(l)}_i$, we have
            
            $$ \begin{aligned}
            \frac{\partial z^{(l)}_i}{\partial a^{(l-1)}_j}
            &= \frac{\partial}{\partial a^{(l-1)}_j} \left( \sum_{k = 1}^{n_{l-1}} a^{(l-1)}_k \theta^{(l-1)}_{i,k} \right) \\
            &= \theta^{(l-1)}_{i,j}
            \end{aligned} $$
            
        \end{proof}
        
        
        
        
        \begin{lemma}
            
            $$
            \frac{\partial \mathbf{z}^{(l)}}{\partial \theta^{(l-1)}_{i,j}} = \begin{pmatrix} \vdots \\ a^{(l-1)}_j \\ \vdots \end{pmatrix} \in \mathbb{R}^{n_l}
            $$
        
        \end{lemma}
        
        \begin{proof}
            
            Using the definition of $\mathbf{z}^{(l)}$,
            
            $$ \begin{aligned}
            \frac{\partial z^{(l)}_i}{\partial \theta^{(l-1)}_{i,j}}
            &= \frac{\partial}{\partial \theta^{(l-1)}_{i,j}} \left( \sum_{k = 0}^{n_{l-1}} \theta^{(l-1)}_{i,k} a^{(l-1)}_j \right) \\
            &= \sum_{k = 0}^{n_{l-1}} \frac{\partial}{\partial \theta^{(l-1)}_{i,j}} \left( \theta^{(l-1)}_{i,k} a^{(l-1)}_j \right) \\
            &= a^{(l-1)}_j
            \end{aligned} $$
            
            since the only non-zero term in the summation is for $k = j$. Note that this is independent of $i$, and therefore
            
            $$
            \frac{\partial \mathbf{z}^{(l)}}{\partial \theta^{(l-1)}_{i,j}} = \begin{pmatrix} \vdots \\ a^{(l-1)}_j \\ \vdots \end{pmatrix} \in \mathbb{R}^{n_l}
            $$
            
        \end{proof}
        
        
        
        
        \begin{theorem}
            
            $$
            \frac{\partial J}{\partial \theta^{(l)}} = \delta^{(l)}_j
            $$
        \end{theorem}
        
        
        






\section{Demonstration of Methods}
\label{sec:demos}

    \subsection{Binary Classification}
    
    \subsection{Multiclass Classification}
        
        % TODO explain why we are only looking at net here, and not logreg
        Although logistic regression is limited to producing a binary output, 
        
        
        
        For the remainder of this section we will consider the Kuzushiji-MNIST (KMNIST) dataset \cite{kmnist}. 
        
        KMNIST is designed as a drop-in replacement for the infamous MNIST dataset of handwritten numeric digits, and therefore is identical in structure to MNIST: each observation is a single 28x28 matrix, where each element represents an "intensity" on a scale of 0 (white) to 1 (black), and therefore when visualised as a matrix plot



\section{Areas for Further Work}    



\bibliography{report}
\bibliographystyle{abbrv}

\end{document}
	
	