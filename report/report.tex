\documentclass{article}
\usepackage{arxiv}

\usepackage{amssymb, amsmath}

\usepackage{graphicx}
\graphicspath{graphics/}

\begin{document}
	
	
	\title{Neural Networks for Classification}
	\author{Owen Jones (olj23\@bath.ac.uk)}
	\date{Spring 2019}
	\maketitle


\begin{abstract}
	Here is the contents of my abstract blah blah blah blah blah.
\end{abstract}



\section{Introduction to Classification Problems}



\section{Logistic Regression}

    \subsection{Introducing Nonlinearity}

    \subsection{The Cost Function}

    \subsection{Regularisation}



\section{Neural Networks}

    \subsection{Multiple Outputs}
    
    \subsection{Forward Propagation}

    \subsection{Backpropagation}
        
        
        $$ \begin{aligned}
        J(\theta) &= - \sum_{i=1}^{n} \left( y_i \log(a^{(L)}_i) + (1 - y_i) \log(1 - a^{(L)}_i) \right) \\
            &= -\mathbf{y}^\top \log(\mathbf{a}^{(L)}) - (\mathbf{1}-\mathbf{y})^\top \log(\mathbf{1} - \mathbf{a}^{(L)})
        \end{aligned} $$
        
        
        
        $$
        \frac{\partial J(\theta)}{\partial \theta^{(L-1)}} =
            \frac{\partial J(\theta)}{\partial \mathbf{a}^{(L)}} \
            \frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{z}^{(L)}} \
            \frac{\partial \mathbf{z}^{(L)}}{\partial \theta^{(L-1)}}
        $$



        $$
        \frac{\partial J(\theta)}{\partial \theta^{(L-2)}} =
            \frac{\partial J(\theta)}{\partial \mathbf{a}^{(L)}} \
            \frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{z}^{(L)}} \
            \frac{\partial \mathbf{z}^{(L)}}{\partial \mathbf{a}^{(L-1)}} \
            \frac{\partial \mathbf{a}^{(L-1)}}{\partial \mathbf{z}^{(L-1)}} \
            \frac{\partial \mathbf{z}^{(L-1)}}{\partial \mathbf{a}^{(L-2)}}
        $$



        $$
        \delta^{(L)} :=
            \frac{\partial J(\theta)}{\partial \mathbf{a}^{(L)}} \
            \frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{z}^{(L)}} \
        $$
        
        
        
        
        $$
        \delta^{(l)} :=
            \delta^{(l+1)} \
            \frac{\partial \mathbf{z}^{(l+1)}}{\partial \mathbf{a}^{(l)}} \
            \frac{\partial \mathbf{a}^{(l)}}{\partial \mathbf{z}^{(l)}} \
        $$
        
        
        
        
        $$ \begin{aligned}
        \frac{\partial J(\theta)}{\partial \mathbf{a}^{(L)}}
            &= \frac{\partial}{\partial \mathbf{a}^{(L)}}
               \left( -\mathbf{y} \log(\mathbf{a}^{(L)}) - (1-\mathbf{y}) \log(1 - \mathbf{a}^{(L)}) \right) \\
            &= \frac{-\mathbf{y}}{\mathbf{a}^{(L)}} - \frac{1 - \mathbf{y}}{1 - \mathbf{a}^{(L)}}
        \end{aligned} $$
        


        $$ \begin{aligned}
        \frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{z}^{(L)}}
            &= \frac{\partial}{\partial \mathbf{a}^{(L)}} \left( \frac{1}{\mathbf{1} + e^{-\mathbf{z}^{(L)}}} \right) \\
            &= \left( - e^{-\mathbf{z}^{(L)}} \right) \left( - \left( \mathbf{1} + e^{-\mathbf{z}^{(L)}} \right)^{-2} \right) \\
            &= \frac{e^{-\mathbf{z}^{(L)}}}{\left( \mathbf{1} + e^{-\mathbf{z}^{(L)}} \right) \left( \mathbf{1} + e^{-\mathbf{z}^{(L)}} \right)} \\
            &= \left( \frac{1}{\mathbf{1} + e^{-\mathbf{z}^{(L)}}} \right)^\top \left( \frac{\left( \mathbf{1} + e^{-\mathbf{z}^{(L)}} \right) - \mathbf{1}}{\mathbf{1} + e^{-\mathbf{z}^{(L)}}} \right)
            &= \left( \mathbf{a}^{(L)} \right)^\top \left( \mathbf{1} - \mathbf{a}^{(L)} \right)
        \end{aligned} $$



\section{Demonstration of Methods}

    \subsection{Binary Classification}
    
    \subsection{Multiclass Classification}
    



\section{Areas for Further Work}    




\section*{References}

	\begin{itemize}
	
		\item[] C. HIGHAM AND D. HIGHAM, \textit{Deep learning: an introduction for applied mathematicians}, arXiv 1801.05894 [cs.LG], (2018).

	\end{itemize}


\end{document}
	
	