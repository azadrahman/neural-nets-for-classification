\documentclass{article}[11pt]
\usepackage{arxiv}
\usepackage{url}

\usepackage{amssymb, amsmath}
\usepackage{amsthm}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\usepackage{algorithm}
\usepackage{algpseudocode}

\setlength{\jot}{12pt}

\usepackage{graphicx}
\graphicspath{graphics/}

\begin{document}
	
	
	\title{Neural Networks for Classification}
	\author{Owen Jones (olj23\@bath.ac.uk)}
	\date{Spring 2019}
	\maketitle


\begin{abstract}
	Here is the contents of my abstract blah blah blah blah blah.
\end{abstract}



\section{Introduction to Classification Problems}

    Historically, machine learning tasks have fallen into one of several categories: supervised learning problems, unsupervised learning problems, and more recently reinforcement learning
    
    Classification tasks (in the sense which this paper considers) belong to the supervised learning family of tasks, where the data used in the training process is \textit{labelled}
    
    qualitative outputs
    
    In Section \ref{sec:demos} of this paper, the 


\section{Logistic Regression}

    $y$ binary output (0 or 1)


    \subsection{Linear Regression}
        
        % TODO intro needs work
        The underlying assumption is always that the output occurs as a result of some combination of inputs.
        
        Mathematically speaking, suppose we have an observation $\mathbf{x}$ consisting of $n \geq 1$ inputs:
        
        $$
        \mathbf{x} = (x_1, \ x_2, \ \ldots, \ x_{n-1}, \ x_n) \in \mathbb{R}^n
        $$
        
        Then we assume that there exists a corresponding set of weights $\boldsymbol{\theta} = (\theta_0, \ldots, \theta_n) \in \mathbb{R}^{n+1}$ such that
        
        $$
        z = \theta_0 + \sum_{k=1}^n \theta_k x_k
        $$
        
        %TODO
        
        To simplify notation later on, we can augment $\mathbf{x}$ with $x_0 = 1$ allowing us to write $z$ as a single summation:
        
        $$
        z = \sum_{k=0}^n \theta_k x_k = \boldsymbol{\theta}^\top \mathbf{x}
        $$
        
        By adjusting the weights $\theta_0, \ldots, \theta_n$, different values of the output $z$ can be produced. Note that if we do not restrict $\boldsymbol{\theta}$ then $z$ is unbounded, i.e. $z \in (-\infty, \infty)$.
        
        
        
    \subsection{Introducing Nonlinearity}
        
        Recall that the goal of a binary classification task is to assign a categorical label to each observation; so if we wish to use linear regression as the basis for our algorithm, ultimately we will need to somehow interpret the continuous output $z \in \mathbb{R}$ as corresponding to one of those discrete labels.
        
        This can be achieved by means of a nonlinear \textit{activation function}:
        
        $$
        \sigma \colon (-\infty, \infty) \to (0, 1)
        $$
        
        In later sections we will consider other activation functions, but for logistic regression we typically use the \textit{sigmoid} function,
            
        $$
        \sigma(z) = \frac{1}{1 + e^{-z}}
        $$
        
        
        % TODO sigmoid plot
        
        
        Note how large positive values of $z$ are approximately mapped to 1, large negative values are approximately mapped to 0, and the transition between the two asymptotic limits is relatively sudden, yet smooth, with the central "threshold" at $z = 0$.
        
        In the remainder of this paper, the activated output $\sigma(z)$ is sometimes denoted as $a$ for the sake of notational simplicity.
        
        
        
    \subsection{The Cost Function}
    \label{sec:costfun}
        
        Now that we have a means of mapping an input $\mathbf{x}$ to an activated output $a \in (0, 1)$, we can begin to think about how to adjust the weights $\boldsymbol{\theta}$ so that $a$ is as close to the target output $y$ as possible.
        
        In order to assess how well the model is performing, we shall require some sort of "penalty value" based on the dissimilarity between $a$ and $y$. For example, the \textit{binary cross-entropy} is defined as follows:
        
        $$
        CE(\boldsymbol{\theta}) = \begin{cases}
            -\log(a) & y = 1 \\
            -\log(1-a) & y = 0
        \end{cases}
        $$
        
        
        % TODO plot
        
        
        Note that $CE$ is a function of $\boldsymbol{\theta}$ (since $a$ is a function of $z$, which is dependent on $\boldsymbol{\theta}$), that it takes a value of 0 if $a = y$, and that it increases exponentially as $a$ becomes further from $y$.
        
        So far we have considered a single observation $\mathbf{x}$ with a label $y$, which in combination with $\boldsymbol{\theta}$ produces a single activated output $a$. However in reality, we have multiple observations (say $m \geq 1$) each with an associated label and each producing an activated output in combination with $\boldsymbol{\theta}$. Therefore when assessing how well $\boldsymbol{\theta}$ parameterises our entire set of observations, we need to consider the cost of each of those observations.
        
        For that purpose, we introduce the \textit{binary cross-entropy cost function}:
        
        $$
        J(\boldsymbol{\theta}) = - \frac{1}{m} \sum_{k=1}^{m} \left( y_k \log(a_k) + (1 - y_k) \log(1 - a_k) \right)
        $$
        
        Since $y_k \in \{0, 1\}$, at most one of the terms in the body of the summation is non-zero for each $k$; and so $J(\boldsymbol{\theta})$ is simply the average cross-entropy across all the observations in our dataset. In other words, $J(\boldsymbol{\theta})$ quantifies how "wrong" the model is, so when adjusting $\boldsymbol{\theta}$ we will be aiming to minimise $J(\boldsymbol{\theta})$.
    
    
    
    \subsection{Regularisation}
        
        % TODO
        
        
    \subsection{Training}
        
        As described at the end of Section \ref{sec:costfun}, the goal of the \textit{training} process is to minimise the cost function $J(\boldsymbol{\theta})$ by making changes to the parameters $\boldsymbol{\theta}$: in other words, we essentially have an optimisation problem, which we can tackle in countless different ways.
        
        Here we will focus on \textit{gradient descent}, an iterative method which takes successive steps converging towards a minimum optimal value. Although many other more sophisticated optimisation algorithms exist, gradient descent is one of the most commonly used optimisation methods due to its simplicity yet relative effectiveness: see \cite{lecun_backprop} for further details.
        
        
        \subsubsection{Descent Direction}
            
            Since we wish to reduce $J(\boldsymbol{\theta})$ by taking some step in parameter space, before doing anything else we must determine the direction in which to take that step.
            
            Consider a perturbation $\Delta \boldsymbol{\theta}$ to our current parameters $\boldsymbol{\theta} \in \mathbb{R}^{n+1}$, and notice that a Taylor expansion yields
            
            $$
            J(\boldsymbol{\theta} + \Delta \boldsymbol{\theta}) = J(\boldsymbol{\theta}) + \sum_{k=0}^{n+1} \frac{\partial J(\boldsymbol{\theta})}{\partial \theta_k} \Delta \boldsymbol{\theta} + \mathcal{O} ((\Delta \boldsymbol{\theta})^2)
            $$
            
            Then ignoring higher-order terms, we have
            
            $$
            J(\boldsymbol{\theta} + \Delta \boldsymbol{\theta}) \approx J(\boldsymbol{\theta}) + \nabla J(\boldsymbol{\theta})^\top \Delta \boldsymbol{\theta}
            $$
            
            Now we greedily choose $\Delta \boldsymbol{\theta}$ such that we reduce $J(\boldsymbol{\theta})$ by as much as possible - i.e. by making $\nabla J(\boldsymbol{\theta})^\top \Delta \boldsymbol{\theta}$ as negative as possible. By the Cauchy-Schwarz inequality, the most negative value of $\mathbf{f}^\top \mathbf{g}$ for any $\mathbf{f}, \mathbf{g} \in \mathbb{R}^{n+1}$ is $- \lVert \mathbf{f} \rVert_2 \lVert \mathbf{g} \rVert_2$, when $\mathbf{f} = -\mathbf{g}$. Therefore we shall choose $\Delta \boldsymbol{\theta} = - \nabla J(\boldsymbol{\theta})$ as our \textit{descent direction}. Note that $\nabla J(\boldsymbol{\theta})^\top (- \nabla J(\boldsymbol{\theta})) = -\lVert \nabla J(\boldsymbol{\theta}) \rVert_2^2 \leq 0$ with equality if and only if $\nabla J(\boldsymbol{\theta}) = 0$, and consequently $J(\boldsymbol{\theta} + \Delta \boldsymbol{\theta}) < J(\boldsymbol{\theta})$ so long as $\nabla J(\boldsymbol{\theta}) \neq 0$.
        
    
    
        \subsubsection{Learning rate}
            
            While it is guaranteed that the "steepest descent" direction we have just chosen, $\Delta \boldsymbol{\theta} = - \nabla J(\boldsymbol{\theta})$, is indeed a descent direction, we must recall at this point that we are using a Taylor series approximation for $J(\boldsymbol{\theta} + \Delta \boldsymbol{\theta})$. Therefore this guarantee only applies in some (potentially very small) radius of the parameter space, centred on $\boldsymbol{\theta}$.
            
            Consequently in order to ensure that we obtain a reduction in $J$, we must scale $\Delta \boldsymbol{\theta}$ by some value $\eta$. This value is called the \textit{learning rate}.
            
            Selecting a learning rate is often a delicate task. Too small a value results in undersized steps in the descent direction, meaning convergence might be slow; too large a value may result in "overshooting" the region where descent is guaranteed, leading to oscillation around the optimum or even to complete divergence.
            
            As such, it is often favourable to use an \textit{adaptive learning rate}: rather than a fixed constant $\eta$, a new value $\eta_k$ is used at each iteration of the algorithm. Many methods exist for determining such adaptive rates, often based on line search methods - see \cite{line_search} and \cite{adam}.
            
            In this paper we will use a relatively simple method based on backtracking line search, where the learning rate is grown by some constant factor at each iteration, and then dramatically decreased when it is too large.
            
            
            \begin{algorithm} \label{alg:grow_and_slash}
                
                \caption{"Grow and slash" learning rate}
                
                \begin{algorithmic}
                
                    \State Choose $\eta_1 > 0$, $\alpha \geq 1$, $\beta \geq 1$ (in practice, $\alpha = 1.1$, $\beta = 2$ work well).
                    
                    \For {iterations $k = 1, ...$ of gradient descent}
                        \While {$J(\boldsymbol{\theta} + \eta_k (\Delta \boldsymbol{\theta})) > J(\boldsymbol{\theta})$}
                            \State $\eta_k \gets \eta_k / \beta$
                        \EndWhile
                        
                        \State $\eta_{k+1} \gets \alpha \eta_k$
                    \EndFor
                    
                \end{algorithmic}
            \end{algorithm}
                    
                
                
            
            
            
            

        



\section{Neural Networks}

    \subsection{Multiple Outputs}
    
    
    
            
    Shifted softmax
    
    \subsection{Forward Propagation}

    \subsection{Backpropagation}
        
        
        $$ \begin{aligned}
        J(\theta)
        &= - \sum_{k=1}^{n} \left( y_k \log(a^{(L)}_k) + (1 - y_k) \log(1 - a^{(L)}_k) \right) \\
        &= -\mathbf{y}^\top \log(\mathbf{a}^{(L)}) - (\mathbf{1}-\mathbf{y})^\top \log(\mathbf{1} - \mathbf{a}^{(L)})
        \end{aligned} $$
        
        
        
        $$
        \frac{\partial J}{\partial \theta^{(L-1)}} =
            \frac{\partial J}{\partial \mathbf{a}^{(L)}} \
            \frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{z}^{(L)}} \
            \frac{\partial \mathbf{z}^{(L)}}{\partial \theta^{(L-1)}}
        $$



        $$
        \frac{\partial J}{\partial \theta^{(L-2)}} =
            \frac{\partial J}{\partial \mathbf{a}^{(L)}} \
            \frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{z}^{(L)}} \
            \frac{\partial \mathbf{z}^{(L)}}{\partial \mathbf{a}^{(L-1)}} \
            \frac{\partial \mathbf{a}^{(L-1)}}{\partial \mathbf{z}^{(L-1)}} \
            \frac{\partial \mathbf{z}^{(L-1)}}{\partial \mathbf{a}^{(L-2)}}
        $$



        Where we define the "error" of layer $l$ as
    
        $$
        \mathbf{\delta}^{(l)} :=  \frac{\partial J}{\partial \mathbf{z}^{(l)}} \in \mathbb{R}^{n_l}
        $$
        
        The $j$th component of this error vector is a measure of the sensitivity of the cost function $J(\theta)$ to the (weighted) input from neuron $j$ in layer $l$. Although it is \textit{not} a direct measure of how much a particular neuron is at fault for mistakes in the network's eventual output, such an explanation is at least valid intuitively, since all partial derivatives must be zero if the cost function is at a minimum.
        
        
        % TODO wording!!!
        Now define recursively... for $l = 1, \ldots, L-1$
        
        $$
        \delta^{(l)} =
        \delta^{(l+1)} \
        \frac{\partial \mathbf{z}^{(l+1)}}{\partial \mathbf{a}^{(l)}} \
        \frac{\partial \mathbf{a}^{(l)}}{\partial \mathbf{z}^{(l)}} \
        $$
        
        
        
        % TODO reword this - binary/multiclass?
        Note that when $J(\theta)$ is the cross-entropy cost function, the error in the final layer $L$ can be calculated as follows:
        
        \begin{lemma}
            
            If $J$ is the \textbf{binomial cross-entropy} cost function, and $\sigma$ is the \textbf{sigmoid} activation function, then for all $j = 1, \ \ldots, \ n_L$
            
            $$
            \delta^{(L)}_j = a^{(L)}_j - y_j
            $$
            
        \end{lemma}
        
        
        \begin{proof}
            
            Observe that the partial derivative of $J$ with respect to $a^{(L)}_i$ (the $i$th component of the activated output $\mathbf{a}^{(L)}$) can be represented as
                    
            $$ \begin{aligned}
            \frac{\partial J(\theta)}{\partial a^{(L)}_i}
            &= \frac{\partial}{\partial a^{(L)}_i} \left( - \sum_{k=1}^{n_L} \left( y_k \log(a^{(L)}_k) + (1 - y_k) \log(1 - a^{(L)}_k) \right) \right) \\
            &= - \frac{y_i}{a^{(L)}_i} + \frac{1 - y_i}{1 - a^{(L)}_i}
            \end{aligned} $$
            
            Consider now the partial derivative of the activated output of layer $L$, with respect to the non-activated output; that is,
            
            $$ \begin{aligned}
            \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j}
            &= \frac{\partial}{\partial z^{(L)}_j} \left( \sigma(z^{(L)}_j) \right)
            &= \frac{\partial}{\partial z^{(L)}_j} \left( \frac{1}{1 + e^{-z^{(L)}_i}} \right)
            \end{aligned} $$
                
            Note that if $i \neq j$ then this is clearly zero. If $i = j$,
            
            $$ \begin{aligned}
            \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j}
            &= \left( - e^{-z^{(L)}_j} \right) \left( - \left( 1 + e^{-z^{(L)}_j} \right)^{-2} \right) \\
            &= \frac{e^{-z^{(L)}_j}}{\left( 1 + e^{-z^{(L)}_j} \right) \left( 1 + e^{-z^{(L)}_j} \right)} \\
            &= \left( \frac{1}{1 + e^{-z^{(L)}_j}} \right) \left( \frac{\left( 1 + e^{-z^{(L)}_j} \right) - 1}{1 + e^{-z^{(L)}_j}} \right) \\
            &= \sigma(z^{(L)}_j) \left( 1 - \sigma(z^{(L)}_j) \right) \\
            &= a^{(L)}_j \left( 1 - a^{(L)}_j \right)
            \end{aligned} $$
            
            Now using the definition of $\boldsymbol{\delta}^{(L)}$ and the chain rule,
            
            $$ \begin{aligned}
            \delta^{(L)}_j &= \frac{\partial J(\theta)}{\partial z^{(L)}_j} \\
            &= \sum_{i=1}^{n_L} \left( \frac{\partial J(\theta)}{\partial a^{(L)}_i} \ \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j} \right) \\
            &= \frac{\partial J(\theta)}{\partial a^{(L)}_j} \ \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j} + \sum_{\substack{i=1 \\ i \neq j}}^{n_L} \left( \frac{\partial J(\theta)}{\partial a^{(L)}_i} \ \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j} \right) \\
            &= \frac{\partial J(\theta)}{\partial a^{(L)}_j} \ \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j} + 0 \\
            &= \left( - \frac{y_j}{a^{(L)}_j} + \frac{1 - y_j}{1 - a^{(L)}_j} \right) a^{(L)}_j \left( 1 - a^{(L)}_j \right) \\
            &= a^{(L)}_j (1 - y_j) - y_j (1 - a^{(L)}_j) \\
            &= a^{(L)}_j - y_j
            \end{aligned} $$
            
        \end{proof}
        
        
        % TODO word nicely
        We now complete a similar proof for the softmax case.
        
        
        \begin{lemma}
            
            If $J$ is the \textbf{multiclass cross-entropy} cost function, and $\sigma$ is the \textbf{softmax} activation function, then for all $j = 1, \ \ldots, \ n_L$
            
            $$
            \delta^{(L)}_j = a^{(L)}_j - y_j
            $$
            
        \end{lemma}
        
        
        \begin{proof}
            
            Firstly note that
            
            $$ \begin{aligned}
            \frac{\partial J(\theta)}{\partial a^{(L)}_i}
            &= \frac{\partial}{\partial a^{(L)}_i} \left( - \sum_{k=1}^{n_L} y_k \log(a^{(L)}_k) \right) \\
            &= - \frac{y_i}{a^{(L)}_i}
            \end{aligned} $$
            
            
            Then using the quotient rule,
            
            $$ \begin{aligned}
            \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j}
            &= \frac{\partial}{\partial z^{(L)}_j} \left( \frac{e^{z^{(L)}_i}}{\sum_{k=1}^{n_L} e^{z^{(L)}_k}} \right) \\
            &= \frac{\left( \frac{\partial \left( e^{z^{(L)}_i}\right)}{\partial z^{(L)}_j} \cdot \sum_{k=1}^{n_L} e^{z^{(L)}_k} \right) - e^{z^{(L)}_i} e^{z^{(L)}_j}}{\left( \sum_{k=1}^{n_L} e^{z^{(L)}_k} \right)^2}
            \end{aligned} $$
            
            \begin{itemize}
            
                \item If $i \neq j$,
            
                $$ \begin{aligned}
                \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j}
                &= \frac{0 - e^{z_i} e^{z_j}}{\left( \sum_{k=1}^{n_L} e^{z^{(L)}_k} \right)^2} \\
                &= - \sigma(z_i) \sigma(z_j) \\
                &= - a^{(L)}_i a^{(L)}_j
                \end{aligned} $$
            
                \item On the other hand, if $i = j$,
            
                $$ \begin{aligned}
                \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j}
                &= \frac{\left( e^{z^{(L)}_j} \cdot \sum_{k=1}^{n_L} e^{z^{(L)}_k} \right) - \left( e^{z^{(L)}_j} \right)^2}{\left( \sum_{k=1}^{n_L} e^{z^{(L)}_k} \right)^2} \\
                &= \frac{e^{z^{(L)}_j}}{\sum_{k=1}^{n_L} e^{z^{(L)}_k}} \left( 1 - \frac{e^{z^{(L)}_j}}{\sum_{k=1}^{n_L} e^{z^{(L)}_k}} \right) \\
                &= \sigma(z^{(L)}_j) \left( 1 - \sigma(z^{(L)}_j) \right) \\
                &= a^{(L)}_j \left( 1 - a^{(L)}_j \right)
                \end{aligned} $$
            
            \end{itemize}
            
            Now once again using the definition of $\boldsymbol{\delta}^{(L)}$ and the chain rule, observe that
            
            $$ \begin{aligned}
            \delta^{(L)}_j &= \frac{\partial J(\theta)}{\partial z^{(L)}_j} \\
            &= \sum_{i=1}^{n_L} \left( \frac{\partial J(\theta)}{\partial a^{(L)}_i} \ \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j} \right) \\
            &= \frac{\partial J(\theta)}{\partial a^{(L)}_j} \ \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j} + \sum_{\substack{i=1 \\ i \neq j}}^{n_L} \left( \frac{\partial J(\theta)}{\partial a^{(L)}_i} \ \frac{\partial a^{(L)}_i}{\partial z^{(L)}_j} \right) \\
            &= - \frac{y_j}{a^{(L)}_j} \left( a^{(L)}_j (1 - a^{(L)}_j) \right) + \sum_{\substack{i=1 \\ i \neq j}}^{n_L} \left( - \frac{y_i}{a^{(L)}_i} \left(- a^{(L)}_i a^{(L)}_j \right) \right) \\
            &= - y_j (1 - a^{(L)}_j) + a^{(L)}_j \left( \sum_{i=1}^{n_L} \left( y_i \right) - y_j \right)
            \end{aligned} $$
            
            This looks slightly complicated, but recall that $\mathbf{y}$ is a one-hot encoded vector, and therefore $\sum_{=1}^{n_L} y_i = 1$. Hence we have
            
            $$ \begin{aligned}
            \delta^{(L)}_j &= - y_j (1 - a^{(L)}_j) + a^{(L)}_j (1 - y_j) \\
            &= a^{(L)}_j - y_j
            \end{aligned} $$
            
        \end{proof}
    
        
        
        \begin{lemma}
            
            For all $i = 1, \ \ldots, \ n_l$ and all $j = 1, \ \ldots, \ n_{l-1}$,
            
            $$
            \frac{\partial z^{(l)}_i}{\partial a^{(l-1)}_j} = \theta^{(l-1)}_{i,j}
            $$
            
        \end{lemma}
        
        \begin{proof}
            
            By the definition of $z^{(l)}_i$, we have
            
            $$ \begin{aligned}
            \frac{\partial z^{(l)}_i}{\partial a^{(l-1)}_j}
            &= \frac{\partial}{\partial a^{(l-1)}_j} \left( \sum_{k = 1}^{n_{l-1}} a^{(l-1)}_k \theta^{(l-1)}_{i,k} \right) \\
            &= \theta^{(l-1)}_{i,j}
            \end{aligned} $$
            
        \end{proof}
        
        
        
        
        \begin{lemma}
            
            For all $i = 1, \ \ldots, \ n_l$ and all $j = 1, \ \ldots, \ n_{l-1}$,
            
            $$
            \frac{\partial z^{(l)}_p}{\partial \theta^{(l-1)}_{i,j}} = \begin{cases}
                0 & p \neq i \\
                a^{(l-1)}_j & p = i
            \end{cases}
            $$
            
        \end{lemma}
        
        \begin{proof}
            
            Using the definition of $z^{(l)}_p$,
            
            $$ \begin{aligned}
            \frac{\partial z^{(l)}_p}{\partial \theta^{(l-1)}_{i,j}}
            &= \frac{\partial}{\partial \theta^{(l-1)}_{i,j}} \left( \sum_{k=1}^{n_{l-1}} \theta^{(l-1)}_{p,k} a^{(l-1)}_k \right) \\
            &= \sum_{k=1}^{n_{l-1}} \frac{\partial}{\partial \theta^{(l-1)}_{i,j}} \left( \theta^{(l-1)}_{p,k} a^{(l-1)}_k \right)
            \end{aligned} $$
            
            The result follows by noting that all terms in the summation, or all-but-one terms if $p = i$, are zero.
            
        \end{proof}
        
        
        
        
        \begin{theorem}
            
            $$
            \frac{\partial J}{\partial \theta^{(l-1)}_{i,j}} = \delta^{(l)}_i a^{(l-1)}_j
            $$
            
        \end{theorem}
        
        \begin{proof}
        
            Using the chain rule, the definition of $\boldsymbol{\delta}^{(l)}$ and the results from the previous lemmas,
            
            $$ \begin{aligned}
            \frac{\partial J}{\partial \theta^{(l-1)}_{i,j}}
            &= \sum_{k=1}^{n_l} \left( \frac{\partial J(\theta)}{\partial z^{(l)}_k} \ \frac{\partial z^{(l)}_k}{\partial \theta^{(l-1)}_{i,j}} \right) \\
            &= \sum_{k=1}^{n_l} \left( \delta^{(l)}_k \frac{\partial z^{(l)}_k}{\partial \theta^{(l-1)}_{i,j}} \right) \\
            &= \delta^{(l)}_i a^{(l-1)}_j
            \end{aligned}$$
        
        \end{proof}
        
        



    \subsection{Improvements to Training}
        
        
        \subsubsection{Stochastic Gradient Descent}
        
        \subsubsection{Validation and Testing}



        \subsubsection{Early Stopping}




\section{Demonstration of Methods}
\label{sec:demos}

    \subsection{Binary Classification}
    
    \subsection{Multiclass Classification}
        
        % TODO explain why we are only looking at net here, and not logreg
        Although logistic regression is limited to producing a binary output, 
        
        
        
        For the remainder of this section we will consider the Kuzushiji-MNIST (KMNIST) dataset \cite{kmnist}. 
        
        KMNIST is designed as a drop-in replacement for the infamous MNIST dataset of handwritten numeric digits, and therefore is identical in structure to MNIST: each observation is a single 28x28 matrix, where each element represents an "intensity" on a scale of 0 (white) to 1 (black), and therefore when visualised as a matrix plot



\section{Areas for Further Work}    



\bibliography{report}
\bibliographystyle{abbrv}

\end{document}
	
	